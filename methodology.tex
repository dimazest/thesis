\chapter{A methodology for robust parameter selection}
\label{sec:methodology}

Experiments with distributional vector space models can be divided into two related categories: one that aims to achieve the highest score on a task, and another that studies the behaviour of certain model parameters. The difference between the two categories can be expressed in the following questions:

\begin{compactitem}
\item What parameter combination gives the highest result? \newcite{baroni-dinu-kruszewski:2014:P14-1} is a representative study of this kind.
\item What parameter instance is superior? For example, the study of \newcite{lapesa-evert:2013:CMCL}.
\end{compactitem}

The first question is applicable in a situation when conceptually different methods are compared, for example, the ``count'' and  ``predict'' methods in \newcite{baroni-dinu-kruszewski:2014:P14-1}. The second question is applicable to a study of the difference of parameters within a conceptual method, for instance, the comparison of neighbour rank and distance measure in predicting semantic priming of \newcite{lapesa-evert:2013:CMCL}.

The co-occurrence information can be used in different ways to build distributional models of meaning \cite{Turney:2010:FMV:1861751.1861756}. This has led to a series of  systematic parameter studies \cite{Bullinaria2007,BullinariaLevy2012,kiela-clark:2014:CVSC,lapesa2014large,TACL570}. All of them explore numerous parameter combinations to report the best scores and derive recommendations for the optimal parameter choice.

\newcite{lapesa2014large} make one step further in studying parameter behaviour by identifying the most influential parameters and their two-way interactions with a linear model, which is fit so that parameters of a vector space model predict its performance. This approach tackles two problems. First of all, it is designed to avoid overfitting.%
\footnote{``Overfitting occurs when classifiers make decisions based on accidental properties of the training set that will lead to errors on the test set (or any new data)'' \newcite{9780262133609}.}
Secondly, it avoids noise in parameter selection revealing the regularities in the parameter behaviour.

The goal of this work is to provide the representative performance numbers of count-based distributional models of meaning---so they could be compared to other semantic models---and to study the general behaviour of vector space parameters and compositional operators---so the compositional operators could be fairly compared. The study is performed systematically using recently developed evaluation datasets for lexical and phrasal meaning representations. We express the goal in two research questions that are related to the categories of studies stated above.
\begin{compactitem}
\item What is the performance limit of the count-based distributional models of meaning?
\item How are the count-based distributional models affected by parameter choices?
\end{compactitem}

\section{Strategies of avoiding overfitting}
\label{sec:avoiding-overfitting}

This work adopts the strategy of \newcite{lapesa2014large} to avoid overfitting and reduce noise in parameter selection. Following them, we use several evaluation datasets one by one. Because different datasets might favour particular models, we also test models on all datasets simultaneously by aggregating model performance scores.

The models are tested on two lexical datasets: SimLex-999 \cite{hill2014simlex} and MEN \cite{Bruni:2014:MDS:2655713.2655714}. These two datasets are chosen because they are larger than other previously used datasets. It has been argued that the score variance is strongly dependent on the size of the evaluation dataset: the larger the dataset the more reliable experiment results are \cite{W16-2502}. SimLex-999 consists of 999 word pairs and MEN consists of 3000 word pairs, making them the largest lexical datasets available to us. Other alike datasets are much smaller in size: 353 word pairs \cite{2002:PSC:503104.503110} and 65 word pairs \cite{Rubenstein:1965:CCS:365628.365657} for example.
% * <sdyck@ualberta.ca> 2016-11-03T04:18:49.491Z:
% 
% > score variance is strongly dependent
% dependent in what way? are results more accurate with larger datasets vs smaller?
% DM: they are more reliable.
% ^.

The three compositional datasets that are employed in this study are KS14 \cite{kartsadrqpl2014}, GS11 \cite{Grefenstette:2011:ESC:2145432.2145580} and PhraseRel (Section~\ref{sec:phraserel}). They consist of phrases with controlled syntax (all of them are subject-verb-object phrases) and cover two relationships between phrases: similarity and relevance.

We test the models on the lexical datasets simultaneously to see whether there are models that perform well on both lexical datasets and thus avoid individual dataset idiosyncrasies. Similarly, we test the models on the phrasal datasets.

The scores on lexical and phrasal datasets are combined to identify a model that is universally good in both lexical and compositional tasks. The model selection procedure is performed in two ways. First, we take the compositional operator into account, so we are able to recommend models that perform well on lexical and phrasal datasets with, for example, addition. Finally, we abstract over the compositional operator and seek a model that achieves competitive results in both lexical and phrasal tasks with all operators.
% * <sdyck@ualberta.ca> 2016-11-03T04:23:43.069Z:
%
% > universally good
% DM: it achieves competitive results in both types of tasks.
% ^.

Because we test the models on several datasets, transfer selected models to the unseen datasets and perform model selection on their combinations (which is another way of avoiding overfitting), we also report the models that performed best in our exhaustive evaluation. This allows us to see whether overfitting actually happens, as we expect that during transfer the models with the highest scores will degrade their performance more than the models selected more conservatively. In the sections to follow, we discuss three parameter selection methods.

\subsection{Best model}

This parameter selection technique chooses the parameters that yield the best result. This method is widely adopted \cite{mitchell-lapata:2008:ACLMain,Grefenstette:2011:ESC:2145432.2145580,milajevs-purver:2014:CVSC,milajevs-EtAl:2014:EMNLP2014}, but, as previously discussed, might be prone to overfitting.
% * <sdyck@ualberta.ca> 2016-11-03T04:27:58.404Z:
%
% > overfitting
%
% Just as a note, it might be beneficial to quickly define overfitting the first time it shows up in the thesis. 
% DM: see the first page of Chapter 3 (Lapesa and Evert ...).
% ^.

\subsection{Cross-validation}

Cross-validation \cite{Ney1997} is a model selection method where parameter selection is based on the average performance of the training splits over several evaluation runs. Leaving-one-out cross validation splits the dasets to $N$ parts. Then $N$ runs are performed where each part is used as a testing split and the rest is used as a training split such that the $n$th run will use the $n$th part as a testing split. The average performance over the N testing splits is reported.

Even though cross-validation avoids overfitting, its performance results are not comparable with the best model selection because they are based on averages over the testing splits. Moreover, existing datasets are not made with such an evaluation in mind \cite{W16-2506}, and there is no common agreement on how the datasets should be split to the training and testing parts.

\subsection{Heuristics}

This parameter selection is based on the average performance of the models where some parameters are fixed.

We look for the average model performance for every dimensionality (for lexical experiments) or for every operator-dimensionality combination (for compositional experiments) and a parameter of interest. Knowing the average performances of the values of the parameter of interest, we choose the value with the highest upper bound of the 0.95 confidence interval, as is done in \newcite{milajevs-sadrzadeh-purver:2016:ACL-SRW}.

Because parameters influence model performance differently, the parameters are processed in order of their ablation \cite{lapesa2014large}. A parameter's ablation is proportional to the reduction of the adjusted $R^2$ score if the parameter is left out.

This method not only avoids overfitting but also yields evaluation results that are comparable with the best-model reports.

\section{Hypotheses}
\label{sec:hypotheses}

To conduct the study, we introduce hypotheses that reflect the current consensus in the field of distributional semantics and facilitate the answering of the research questions.

\begin{hyp}[H\ref{hyp:overfitting}]
\label{hyp:overfitting}
Heuristics-based model selection should avoid overfitting.
\end{hyp}

Models that are chosen using heuristics achieve better results than the best models on the datasets they were not instantiated on.

\begin{hyp}[H\ref{hyp:10percent}]
\label{hyp:10percent}
The relative difference between the score of the best model and the score of the model selected using heuristics should be less than 10\%.
\end{hyp}

The optimal results reported in \newcite[\textcolor{citecolor}{Table~5}]{lapesa2014large} are within the 10\% margin (with an exception of the ESSLLI dataset, where the margin is 21\%). We expect similar relative differences in our results.

\begin{hyp}[H\ref{hyp:var}]
\label{hyp:var}
High dimensional models should be more likely to perform better than their low dimensional counterparts.
\end{hyp}

As the vector space dimensionality increases, performance stabilises \cite{kiela-clark:2014:CVSC,BullinariaLevy2012,lapesa2014large}. We speculate that in a highly dimensional case the difference between parameter choices matters less and thus higher results are reported more often for highly-dimensional spaces.

\begin{hyp}[H\ref{hyp:dimen}]
\label{hyp:dimen}
The optimal parameter choice should depend on dimensionality.
\end{hyp}

\newcite{milajevs-sadrzadeh-purver:2016:ACL-SRW} showed that the parameter recommendations provided by \newcite{TACL570} are not applicable to the vector spaces with a dimensionality of a few thousand.

The difference could be because the co-occurrence counts of the most frequent pairs do not contain noise. The counts (and therefore the probability estimates) of less frequent pairs are noisy and require a special treatment to compensate for PMI's Achilles heel when small co-occurrence counts lead to extremely high PMI values \cite{TACL570}.

\begin{hyp}[H\ref{hyp:not-lextocomp}]
\label{hyp:not-lextocomp}
Models that perform well on lexical tasks also perform well on compositional tasks.
\end{hyp}

\begin{hyp}[H\ref{hyp:order}]
\label{hyp:order}
The best models for compositional tasks should take word order into account.
\end{hyp}

It has been shown that addition performs very well in compositional tasks \cite{milajevs-EtAl:2014:EMNLP2014}, but that could be because optimal parameters for other operators were not chosen.

\begin{hyp}[H\ref{hyp:universal}]
\label{hyp:universal}
There should be a universal model that performs well on a broad range of tasks.
\end{hyp}

We are interested to see whether there is one parameter choice that performs competitively on all tasks.

\chapter{Parameters and experimental setup}
\label{sec:parameters}

This chapter explains in detail the parameters that are explored in the experiments. The core of the parameters are the parameters that modify the co-occurrence quantification. The other parameters define the dimensionality of the vector space, the similarity measure and compositional operator.
% * <sdyck@ualberta.ca> 2016-11-03T04:59:10.670Z:
%
% > f them
%
% core of what? the experiments?
% DM: parameters.
% ^.

\section{Co-occurrence quantification}
\label{sec:quantification}

\subsection{PMI variants (discr)}
\label{sec:pmi-variants}

Most co-occurrence weighting schemes%
\footnote{We abbreviate this parameter as \texttt{discr} because the weighting scheme discriminates the features.
% TODO: refer to the ir-dataset paper.
}
in distributional semantics are based on \emph{point-wise mutual information} (PMI, Equation~\ref{eq:pmi}, \newcite{J90-1003,Turney:2010:FMV:1861751.1861756,NIPS2014_5477}).
%
\begin{equation}
  \label{eq:pmi}
  \operatorname{PMI}(x, y) = \log\frac{P(x,y)}{P(x)P(y)}
\end{equation}
%
PMI in its raw form is problematic: non-observed co-occurrences lead to infinite PMI values, making it impossible to compute similarity. A common solution to this problem is to replace all infinities with zeros, and we use PMI hereafter to refer to a weighting with this fix. Refer to Section~\ref{sec:quantification-measures} for a discussion.

An alternative solution is to increment the probability ratio by 1; we refer to weighting scheme as \emph{compressed PMI} (CPMI):
%
\begin{equation}
  \label{eq:cpmi}
  \operatorname{CPMI}(x, y) = \log\left( 1 +  \frac{P(x,y)}{P(x)P(y)} \right)
\end{equation}

\subsection{Shifted PMI (neg)}
\label{sec:shifted-pmi}

Many approaches use only \emph{positive} PMI values, as  negative PMI values may not positively contribute to model performance \cite{Turney:2010:FMV:1861751.1861756}. This can be generalised to an additional cutoff parameter $k$ (abbreviated as \texttt{neg}) following \newcite{TACL570}, giving our third PMI variant: \emph{shifted PMI} or SPMI for short:
%
\begin{equation}
  \label{eq:ppmi}
  \operatorname{SPMI_k} = \max (0, \operatorname{PMI}(x, y) - \log k)
\end{equation}
%
We can apply the same idea to CPMI and obtain \emph{shifted compressed PMI} or SCPMI:
%
\begin{equation}
  \label{eq:pcpmi}
  \operatorname{SCPMI_k} = \max (0, \operatorname{CPMI}(x, y) - \log 2k)
\end{equation}

\subsection{Frequency weighting (freq)}
\label{sec:frequency-weighting}

Another issue with PMI is its bias toward rare events \cite{TACL570}; one way of solving this issue is to weight the value by the co-occurrence frequency obtaining the \emph{local mutual information} measure \cite{Evert05}:
%
\begin{equation}
  \label{eq:lmi}
  \operatorname{LMI}(x, y) = n(x, y)\operatorname{PMI}(x, y)
\end{equation}
%
where $n(x, y)$ is the number of times $x$ was seen together with $y$. For clarity, we do not use the LMI term and we refer to $n$-weighted PMIs as \NPMI/, \NSPMI/, etc. When this weighting component is set to 1, it has no effect; we can explicitly label it as \PMI/, \SPMI/, etc. In addition to the extreme $1$ and $n$ weightings, we also experiment with the $\log n$ weighting. We refer to this parameter as \texttt{freq}.

\subsection{Context distribution smoothing (cds)}
\label{sec:cont-distr-smooth}

\newcite{TACL570} show that performance is affected by smoothing the context distribution $P(x)$:
%
\begin{equation}
  \label{eq:cds}
  P_{\alpha}(x) = \frac{n(x)^{\alpha}}{\sum_{f \in F}n(f)^{\alpha}}
\end{equation}
where $n(x)$ is the frequency of the term $x$, F is the set of the features in the co-occurrence matrix and $n(f)$ is the frequency of the feature in the corpus. We experiment with $\alpha=1$ (no smoothing) and $\alpha = 0.75$. We call this estimation method \emph{local context probability}.

We also estimate a \emph{global context probability} based on the size of the corpus $|C|$:
%
\begin{equation}
  \label{eq:cds-nan}
  P(x) = \frac{n(x)}{|C|}
\end{equation}

\subsection{Quantification measure generalisation}
\label{sec:quantification-measures}

To systematically study the aforementioned quantification measures, together with other variations, we propose to view all these measures as instances of this general formula:
%
\begin{equation}
  \small
  \label{eq:association}
  \operatorname{Quantification}(x,y) = \operatorname{freq}(x, y)
                                       \operatorname{discr}(x, y)
\end{equation}
%
which consists of two components: $\operatorname{freq}(x, y)$ which quantifies the co-occurrence of two terms---a target term $x$ and a feature term $y$; and $\operatorname{discr}(x, y)$ which quantifies the ``surprise'' or ``informativeness'' of seeing (or not seeing) the two terms together, labeled as discriminativeness.

In this framework, PMI can be seen as a quantification measure where the frequency component is the constant 1 and the discriminativeness is the PMI itself, SPMI is seen analogously. For \NPMI/, $\operatorname{freq}(x, y) = n(x, y)$ and $\operatorname{discr}(x, y) = \operatorname{PMI}(x, y)$.

From the probabilistic point of view, under the independence assumption of two words occurring together, \NPMI/ can be interpreted as measuring the logarithm of the ratio of the probabilities of groups of length $n$ (the group that contains only pairs of $(x,y)$s and another one that contains $x$s and $y$s):
%
\begin{equation*}
  n\log\frac{P(x, y)}{P(x)P(y)} = \log\frac{P(x, y)^{n}}{P(x)^{n}P(y)^{n}}
\end{equation*}
%
% When the constant 1 is used as the frequency component, it is equivalent to the subsumption assumption (the probability of a sequence depends only on what elements it contains, rather than how many of them).
% * <sdyck@ualberta.ca> 2016-11-03T05:38:23.892Z:
%
% >  when the probability of a sequence depends only on what elements it contains, rather than how many of them.
%
% is this the definition of the subsumption assumption, or is it a condition needed for the frequency component to be equivalent to it?
% DM: it's the definition. This is actually confusing to everyone I've shown this to, apart from someone who gave me this idea.
% ^.

From the geometric point of view, the transformation from \PMI/ to \NPMI/ changes the directions of vectors by pulling the vectors toward the dimensions for which $n(x, y)$ is higher. As a side effect, it also stretches the vectors. The importance of these two effects is discussed later in Section~\ref{sec:similarity-measure}.

From the linguistic perspective, \PMI/ captures the tendency for a word to co-occur with another word in general (captured by the direction of a vector), while \NPMI/ captures the expectation of seeing a particular co-occurrence in the source corpus. This is encoded in both the direction and the length of a vector.

The sublinear frequency\footnote{We increment $n$ by one to avoid an infinite logarithm value when $n$ is 0.} $\log n$ stands between the two extreme cases of independence and subsumption. However, it is still a strong assumption as it treats all word pairs equally, but in natural language there are some pairs that are closer to subsumption and others that are closer to independence.

In our case, the constant frequency is the only association measure that is able to distinguish from the case where a co-occurrence event was not observed (and so according to MLE, $P(x, y)= 0$) and the case where the events are independent ($P(x, y) = P(x)P(y)$). For other frequencies, when no co-occurrence is observed, the frequency is 0 and the discriminativeness value does not matter unless it is finite. That is why the backing off strategy already mentioned in Section~\ref{sec:pmi-variants}, ``when $n(x, y) = 0$ assume that $P(x, y) = P(x)P(y)$'' is an appropriate way of avoiding $-\infty$. There are many other smoothing strategies, two examples of which are \newcite{kneser1995improved,bengio2006}.

\section{Other model parameters}
\label{sec:other-model-paramt}

\input{figures/parameters}

The source corpus that we use is the concatenation of ukWaC and Wackypedia \cite{ukwac}.\footnotemark{} A window of 5 neighbour words from each side is used to collect co-occurrences.
% * <sdyck@ualberta.ca> 2016-11-03T05:48:30.822Z:
%
% > A symmetric widow of 5 neighbouring is
%
% This could just be my lack of familiarity with the specific subject matter, but this doesn't make sense to me!
% DM: it's a window not a widow :)
% ^.

\footnotetext{The ukWaC corpus is available at \url{http://wacky.sslmit.unibo.it}.}

\subsection{Vector dimensionality (D)}
\label{sec:vect-dimens}

As context words we select the 1K, 2K, 3K, 5K, 10K, 20K, 30K, 40K and 50K most frequent lemmatised nouns, verbs, adjectives and adverbs in the source corpus. All context words are part-of-speech tagged, but we do not distinguish between refined word types (e.g.~intransitive vs.~transitive versions of verbs).

\subsection{Similarity measure}
\label{sec:similarity-measure}

To be able to measure the similarity of two words, we need to be able to compare their vectors. A very high-level approach is to look at how two words agree on their features. If two word vectors tend to have equal values for most of their components, then this is a good indication of the similarity of the words they represent.

The cosine of the angle between two vectors is a widely used similarity measure in distributional semantics \cite{Turney:2010:FMV:1861751.1861756,lapesa2014large}.
%
\begin{equation*}
  \label{eq:cos}
  \cos(\vec{x}, \vec{y}) = \frac{\vec{x} \cdot \vec{y}}
                                {\|\vec{x}\| \|\vec{y}\|}
\end{equation*}

However, the inner product $\vec{x} \cdot \vec{y}$ is preferred in information retrieval and current state-of-the-art natural language processing (NLP) systems \cite{mikolov2013distributed,mikolov2013linguistic,TACL570}. The cosine of the angle is the inner product of the normalised vectors (using Euclidean $L_2$ length).
% * <sdyck@ualberta.ca> 2016-11-04T02:11:45.531Z:
%
% > IR and current state-of-the-art NLP
%
% I would possibly state was these acronyms stand for (if they are in fact acronyms)
% DM: in fact, they are.
% ^.

Normalisation reduces all vectors to unit length leaving their directions to characterise them. Thus, remembering that vector length depends on overall frequency, linguistically we have two measures: the cosine measure that is concerned with similarity, and inner product with no normalisation which in addition to similarity also reflects word frequency and expectation factors.
% * <sdyck@ualberta.ca> 2016-11-04T02:13:38.667Z:
%
% > also
%
% so, the inner product is concerned with similarity, just as the cosine is, as well as the word frequency/expectation factors?
% DM: just as well.
% ^ <sdyck@ualberta.ca> 2016-11-04T02:15:23.212Z.

An advantage of cosine in a lexical similarity task is that it does not depend on the word frequency. Imagine a situation where the similarity of a frequent and a rare word is calculated. Then the similarity judgment should not depend on the relative frequency of the words; instead, their tendency of agreement on features should take the dominant role.

For example, \NPMI/ makes ``feature selections'' by weighting PMI values with the co-occurrence frequency, as discussed in Section~\ref{sec:quantification-measures}. When cosine is applied, the stretching effect of \NPMI/ is eliminated, but the rotational effect stays. On average, the rotational effect will be much more significant for rare words, while frequent words are more likely to be stretched.

In addition to cosine and inner product, we use correlation \cite{kiela-clark:2014:CVSC} to measure similarity.

\subsection{Compositional operator}
\label{sec:comp-oper}

\input{figures/comp-methods}

For phrasal tasks, the phrase vectors are obtained via composition of the phrase constituents' vectors using addition, multiplication \cite{mitchell2010composition,mitchell-lapata:2008:ACLMain}, Kronecker \cite{Grefenstette:2011:ESC:2145432.2145580} and categorical operators listed in Table~\ref{tbl:comp-methods} \cite{milajevs-EtAl:2014:EMNLP2014,DBLP:journals/corr/abs-1003-4394}.

Addition and multiplication are point-wise operations and do not take into account word order. Kronecker and categorical operators are sensitive to word ordering. The verb matrix for Kronecker is defined as the Kronecker product of the vector of a verb with itself:
%
\begin{equation*}
  \widetilde{\text{Verb}} = \overrightarrow{\text{Verb}} \otimes \overrightarrow{\text{Verb}}
\end{equation*}

For categorical operators, a transitive verb is represented by matrix $\overline{\text{Verb}}$, which is obtained from a corpus using the formula:
$$
\overline{\text{Verb}} = \sum_i\ov{s_i} \otimes \ov{o_i}
$$
where $\ov{s_i}$ and $\ov{i_i}$ are the subject-object pairs of the verb found in the source corpus.

As a non-compositional baseline, we take the condition \texttt{head}, which ignores the subject and the object of a phrase, causing the vector of a whole phrase to be equal to the vector of its verb \cite{milajevs-EtAl:2014:EMNLP2014}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
