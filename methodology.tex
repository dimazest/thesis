\chapter{A methodology for robust parameter selection}
\label{sec:methodology}

Experiments with distributional vector space models can be divided into two classes. One class aims to achieve the highest score on a task. Another class studies the behaviour of certain model parameters. The difference between the two classes can be expressed in the following questions:

\begin{compactitem}
\item What parameter combination gives the highest result? \newcite{baroni-dinu-kruszewski:2014:P14-1} is a representative study of this kind.
\item Does a newly proposed technique outperform existing methods? For example, the study of \newcite{lapesa-evert:2013:CMCL}, which contrasts ranked-based semantic priming estimation with distance-based.
\end{compactitem}

The first question is applicable in a situation when conceptually different methods are compared, for example, the ``count'' and  ``predict'' methods in \newcite{baroni-dinu-kruszewski:2014:P14-1} or when the best performance score is required. The second question is applicable to a study of the difference in performance of parameters within a conceptual method, for instance, the comparison of neighbour rank and distance measure in predicting semantic priming of \newcite{lapesa-evert:2013:CMCL}, where the goal is not to identify the best model, but to contrast parameter instances.

The co-occurrence information can be used in different ways to build distributional models of meaning \cite{Turney:2010:FMV:1861751.1861756}. This has led to a series of  systematic parameter studies \cite{Bullinaria2007,BullinariaLevy2012,kiela-clark:2014:CVSC,lapesa2014large,TACL570,baroni-dinu-kruszewski:2014:P14-1}. All of them explore numerous parameter combinations to report the best scores and derive recommendations for the optimal parameter choice.

\newcite{lapesa2014large} make one step further in studying parameter behaviour by identifying the most influential parameters and their two-way interactions with a linear model, which is fitted so that parameters of a vector space model predict the performance of the vector space model on a task. It avoids iterative parameter tuning by testing all possible parameter combinations, so that unknown parameter interactions are captured. They avoid overfitting\footnotemark{} and noise in the data by using a linear regression model.

\footnotetext{``Overfitting occurs when classifiers make decisions based on accidental properties of the training set that will lead to errors on the test set (or any new data)'' \newcite{9780262133609}.}

The first goal of this work is to provide the representative performance measures of count-based distributional models of meaning---so that they could be compared to other semantic models. The second and the main goal is to study the general behaviour of vector space parameters and compositional operators---so that the compositional operators could be fairly compared. The study is performed systematically using recently developed evaluation datasets for lexical and phrasal similarity. We express the goal in two research questions that are related to the categories of studies stated above.
\begin{compactitem}
\item What is the performance limit of distributional models of meaning?
\item How do compositional operators and lexical representations affect one another?
\end{compactitem}

\section{Strategies for avoiding overfitting}
\label{sec:avoiding-overfitting}

This work adopts the strategy of \newcite{lapesa2014large} to avoid overfitting and reduce noise in parameter selection. Following them, we use several evaluation datasets one by one. In this case we are able to identify cases when a model behaves particularly well on one dataset, but poorly on another, which is an example of overfitting.

To be able to identify models that perform well on several datasets, we also test the models on all datasets simultaneously by aggregating model performance scores. We do not aggregate dataset entries by, for example, taking a union of all of the entries in them because the judgements are on different scales and the participants were given different instruction.

The models are tested on two word similarity datasets: SimLex-999 \cite{hill2014simlex} and MEN \cite{Bruni:2014:MDS:2655713.2655714}. These two datasets are chosen because they are larger than other previously used datasets. \citet{W16-2502} argue that the score variance is strongly dependent on the size of the evaluation dataset: the larger the dataset the more reliable experiment results are. SimLex-999 consists of 999 word pairs and MEN consists of 3\,000 word pairs, making them the largest lexical datasets available to us. Other alike datasets are much smaller in size: 353 word pairs \cite{2002:PSC:503104.503110} and 65 word pairs \cite{Rubenstein:1965:CCS:365628.365657} for example.

The three phrasal  datasets that are employed in this study are KS14 \cite{kartsadrqpl2014}, GS11 \cite{Grefenstette:2011:ESC:2145432.2145580} and PhraseRel (Section~\ref{sec:phraserel}). They consist of phrases with controlled syntax (all of them are subject-verb-object phrases) and cover two relationships between phrases: similarity and relevance.

We test the models on the lexical datasets simultaneously to see whether there are models that perform well on both lexical datasets and thus avoid individual dataset characteristics that are independent to the phenomena of interest (similarity, relatedness or relevance). Similarly, we test the models on the phrasal datasets.

The scores on lexical and phrasal datasets are combined to identify a model that is universally good in lexical and compositional tasks. The model selection procedure is performed in two ways. First, we take the compositional operator into account, so we are able to recommend models that perform well on lexical and phrasal datasets with addition, multiplication and Kronecker (see Section~\ref{sec:composition} for the description of compositional operators). Finally, we abstract over the compositional operator and seek a model that achieves competitive results in both lexical and phrasal tasks with all operators.

We test the models on several datasets, transfer selected models to the unseen datasets and perform model selection on their combinations to avoid overfitting and obtain reliable model performance measurements.

In addition to that, we also report the results of the models that performed best in our exhaustive evaluation of testing all possible parameter combinations. This allows us to see whether overfitting actually happens, as we expect that during transfer the models with the highest scores will degrade in performance to a greater extent than the models selected more conservatively.

In the sections to follow, we discuss three parameter selection methods that we have applied.

\subsection{Best model}

This parameter selection technique chooses the parameters that yield the best result. This method is widely adopted.
However, as previously discussed, it might be prone to overfitting.

\subsection{Cross-validation}

Cross-validation \cite{Ney1997} is a widely used model selection method where parameter selection is based on the average performance of the training splits over several evaluation runs. Leaving-one-out cross validation splits the dasets to $N$ parts. Then $N$ runs are performed where each part is used as a testing split and the rest is used as a training split such that the $n$th run will use the $n$th part as a testing split. Training splits are used to tune parameters. The average performance over the $N$ testing splits is reported. Note that different model parameters might be used on testing splits.

Even though cross-validation avoids overfitting, its performance results are not comparable with the best model selection because they are based on averages over the testing splits. Moreover, existing datasets are not made with such an evaluation in mind \cite{W16-2506}, and there is no common agreement on how the datasets should be split to the training and testing parts.

\subsection{Heuristics}

This parameter selection is based on the average performance of the models where some parameters are fixed.

We look for the average model performance for every dimensionality (for lexical experiments) or for every operator-dimensionality combination (for compositional experiments) and a parameter of interest. Knowing the average performances of the values of the parameter of interest, we choose the value with the highest upper bound of the 0.95 confidence interval.

Because parameters influence model performance differently, the parameters are processed in order of their ablation \cite{lapesa2014large}. A parameter's ablation is proportional to the reduction of the adjusted $R^2$ scores between a linear model that treats all parameters as independent variables and a linear model that leaves out the parameter of interest from the independent variables.

This method not only avoids overfitting but also yields evaluation results that are comparable with the best-model reports.

\section{Parameters}
\label{sec:parameters}

This section explains in detail the parameters that are explored in the experiments. The core of the parameters are the parameters that modify the co-occurrence frequencies. The other parameters define the dimensionality of the vector space, the similarity measure and compositional operator.

\subsection{Co-occurrence quantification}
\label{sec:quantification}

\subsubsection{PMI variants (discr)}
\label{sec:pmi-variants}

Most co-occurrence weighting schemes%
\footnote{We abbreviate this parameter as \texttt{discr} because the weighting scheme discriminates the features.
% TODO: refer to the ir-dataset paper.
}
in distributional semantics are based on \emph{point-wise mutual information} (PMI, Equation~\ref{eq:pmi}, \newcite{J90-1003,Turney:2010:FMV:1861751.1861756,NIPS2014_5477}).
%
\begin{equation}
  \label{eq:pmi}
  \operatorname{PMI}(x, y) = \log\frac{P(x,y)}{P(x)P(y)}
\end{equation}
%
PMI in its raw form is problematic: non-observed co-occurrences lead to infinite PMI values, making it impossible to compute similarity. A common solution to this problem is to replace all infinities with zeros, and we use PMI hereafter to refer to a weighting with this fix.

An alternative solution is to increment the probability ratio by 1; we refer to weighting scheme as \emph{compressed PMI} (CPMI):
%
\begin{equation}
  \label{eq:cpmi}
  \operatorname{CPMI}(x, y) = \log\left( 1 +  \frac{P(x,y)}{P(x)P(y)} \right)
\end{equation}

Another issue with PMI is its bias towards rare events. Consider a context $c_r$  with very low probability (it could be a rare context word, a tokenization error or a misspelled word). The probability $P(c_r)$ will be much lower than for other context, at the same time the PMI values for that feature will be higher. It is the same for rare target words. We refer to this issue as PMI'a \textit{Achilles heel}.

\subsubsection{Shifted PMI (neg)}
\label{sec:shifted-pmi}

Many approaches use only \emph{positive} PMI values, as  negative PMI values may not positively contribute to model performance \cite{Turney:2010:FMV:1861751.1861756}. This can be generalised to an additional cutoff parameter $k$ (abbreviated as \texttt{neg}) following \newcite{TACL570}, giving our third PMI variant: \emph{shifted PMI} or SPMI for short:
%
\begin{equation}
  \label{eq:ppmi}
  \operatorname{SPMI_k} = \max (0, \operatorname{PMI}(x, y) - \log k)
\end{equation}
%
We can apply the same idea to CPMI and obtain \emph{shifted compressed PMI} or SCPMI:
%
\begin{equation}
  \label{eq:pcpmi}
  \operatorname{SCPMI_k} = \max (0, \operatorname{CPMI}(x, y) - \log 2k)
\end{equation}

\subsubsection{Frequency weighting (freq)}
\label{sec:frequency-weighting}

%Another issue with PMI is its bias toward rare events \cite{TACL570};
One way of solving PMI's bias toward rare events is to weight the value by the co-occurrence frequency obtaining the \emph{local mutual information} (LMI, \newcite{Evert05}), for clarity we refer to LMI as \NPMI/:
%
\begin{equation}
  \label{eq:lmi}
  \operatorname{nPMI}(x, y) = n(x, y)\operatorname{PMI}(x, y)
\end{equation}
%
where $n(x, y)$ is the number of times $x$ was seen together with $y$. We refer to $n$-weighted PMIs as \NPMI/, \NSPMI/, etc. When this weighting component is set to 1, it has no effect; we can explicitly label it as \PMI/, \SPMI/, etc. In addition to the extreme $1$ and $n$ weightings, we also experiment with the $\log n$ weighting. We refer to this parameter as \texttt{freq}.

\subsubsection{Context distribution smoothing (cds)}
\label{sec:cont-distr-smooth}

\newcite{TACL570} show that performance is affected by smoothing the context distribution $P(x)$:
%
\begin{equation}
  \label{eq:cds}
  P_{\alpha}(x) = \frac{n(x)^{\alpha}}{\sum_{f \in F}n(f)^{\alpha}}
\end{equation}
where $n(x)$ is the frequency of the term $x$, F is the set of the features in the co-occurrence matrix and $n(f)$ is the frequency of the feature in the corpus. We experiment with $\alpha=1$ (no smoothing) and $\alpha = 0.75$. We call this estimation method \emph{local context probability}.

We also estimate a \emph{global context probability} based on the size of the corpus $|C|$:
%
\begin{equation}
  \label{eq:cds-nan}
  P(x) = \frac{n(x)}{|C|}
\end{equation}

\subsubsection{Quantification measure generalisation}
\label{sec:quantification-measures}

To systematically study the aforementioned quantification measures, together with other variations, we propose to view all these measures as instances of this general formula:
%
\begin{equation}
  \small
  \label{eq:association}
  \operatorname{Quantification}(x,y) = \operatorname{freq}(x, y)
                                       \operatorname{discr}(x, y)
\end{equation}
%
which consists of two components: $\operatorname{freq}(x, y)$ which quantifies the co-occurrence of two terms---a target term $x$ and a feature term $y$; and $\operatorname{discr}(x, y)$ which quantifies the ``surprise'' or ``informativeness'' of seeing (or not seeing) the two terms together, labeled as discriminativeness.

In this framework, PMI can be seen as a quantification measure where the frequency component is the constant 1 and the discriminativeness is the PMI itself. SPMI, CPMI and SCPMI are seen analogously. For \NPMI/, $\operatorname{freq}(x, y) = n(x, y)$ and $\operatorname{discr}(x, y) = \operatorname{PMI}(x, y)$.

From the probabilistic point of view, under the independence assumption of two words occurring together, \NPMI/ can be interpreted as measuring the logarithm of the ratio of the probabilities of groups of length $n$ (the group that contains only pairs of $(x,y)$s and another one that contains $x$s and $y$s):
%
\begin{equation}
  n\log\frac{P(x, y)}{P(x)P(y)} = \log\frac{P(x, y)^{n}}{P(x)^{n}P(y)^{n}}
\end{equation}
%
% When the constant 1 is used as the frequency component, it is equivalent to the subsumption assumption (the probability of a sequence depends only on what elements it contains, rather than how many of them).
% * <sdyck@ualberta.ca> 2016-11-03T05:38:23.892Z:
%
% >  when the probability of a sequence depends only on what elements it contains, rather than how many of them.
%
% is this the definition of the subsumption assumption, or is it a condition needed for the frequency component to be equivalent to it?
% DM: it's the definition. This is actually confusing to everyone I've shown this to, apart from someone who gave me this idea.
% ^.

From the geometric point of view, the transformation from \PMI/ to \NPMI/ changes the directions of vectors by pulling the vectors toward the dimensions for which $n(x, y)$ is higher. As a side effect, it also stretches the vectors.% The importance of these two effects is discussed later in Section~\ref{sec:similarity-measure}.

From the linguistic perspective, \PMI/ captures the tendency for a word to co-occur with another word in general (captured by the direction of a vector), while \NPMI/ captures the expectation of seeing a particular co-occurrence in the source corpus. This is encoded in both the direction and the length of a vector.

% The sublinear frequency\footnote{We increment $n$ by one to avoid an infinite logarithm value when $n$ is 0.} $\log n$ stands between the two extreme cases of independence and subsumption. However, it is still a strong assumption as it treats all word pairs equally, but in natural language there are some pairs that are closer to subsumption and others that are closer to independence.

% In our case, the constant frequency is the only association measure that is able to distinguish from the case where a co-occurrence event was not observed (and so according to MLE, $P(x, y)= 0$) and the case where the events are independent ($P(x, y) = P(x)P(y)$). For other frequencies, when no co-occurrence is observed, the frequency is 0 and the discriminativeness value does not matter unless it is finite. That is why the backing off strategy already mentioned in Section~\ref{sec:pmi-variants}, ``when $n(x, y) = 0$ assume that $P(x, y) = P(x)P(y)$'' is an appropriate way of avoiding $-\infty$. There are many other smoothing strategies, two examples of which are \newcite{kneser1995improved,bengio2006}.

\subsection{Other model parameters}
\label{sec:other-model-paramt}

\input{figures/parameters}

The source corpus that we use is the concatenation of ukWaC and Wackypedia \cite{ukwac}.\footnotemark{} A window of 5 neighbour words from each side is used to collect co-occurrences.
% * <sdyck@ualberta.ca> 2016-11-03T05:48:30.822Z:
%
% > A symmetric widow of 5 neighbouring is
%
% This could just be my lack of familiarity with the specific subject matter, but this doesn't make sense to me!
% DM: it's a window not a widow :)
% ^.

\footnotetext{The ukWaC corpus is available at \url{http://wacky.sslmit.unibo.it}.}

\subsubsection{Vector dimensionality (D)}
\label{sec:vect-dimens}

As context words we select the 1K, 2K, 3K, 5K, 10K, 20K, 30K, 40K and 50K most frequent lemmatised nouns, verbs, adjectives and adverbs in the source corpus. All context words are part-of-speech tagged, but we do not distinguish between refined word types (e.g.~intransitive vs.~transitive versions of verbs).

\subsubsection{Similarity measure}
\label{sec:similarity-measure}

To be able to measure the similarity of two words, we need to be able to compare their vectors.\footnotemark{} A very high-level approach is to look at how two words agree on their features. If two word vectors tend to have approximately equal values for most of their components, then this is a good indication of the similarity of the words they represent.

\footnotetext{Even though similarity is not strictly a parameter of a distributional model, it is treated the same as a model parameter.}

The cosine of the angle between two vectors is a widely used similarity measure in distributional semantics \cite{Turney:2010:FMV:1861751.1861756,lapesa2014large}.
%
\begin{equation}
  \label{eq:cos}
  \cos(\vec{x}, \vec{y}) = \frac{\vec{x} \cdot \vec{y}}
                                {\|\vec{x}\| \|\vec{y}\|}
\end{equation}

However, the inner product $\vec{x} \cdot \vec{y}$ is preferred in information retrieval and current state-of-the-art natural language processing systems \cite{mikolov2013distributed,mikolov2013linguistic,TACL570}. The cosine of the angle is the inner product of the normalised vectors (using Euclidean $L_2$ length).
% * <sdyck@ualberta.ca> 2016-11-04T02:11:45.531Z:
%
% > IR and current state-of-the-art NLP
%
% I would possibly state was these acronyms stand for (if they are in fact acronyms)
% DM: in fact, they are.
% ^.

Normalisation reduces all vectors to unit length leaving their directions to characterise them. Thus, remembering that vector length depends on overall frequency, linguistically we have two measures: the cosine measure that is concerned with similarity, and inner product with no normalisation which in addition to similarity also reflects word frequency and expectation factors.
% * <sdyck@ualberta.ca> 2016-11-04T02:13:38.667Z:
%
% > also
%
% so, the inner product is concerned with similarity, just as the cosine is, as well as the word frequency/expectation factors?
% DM: just as well.
% ^ <sdyck@ualberta.ca> 2016-11-04T02:15:23.212Z.

An advantage of cosine in a lexical similarity task is that it does not depend on the word frequency. Imagine a situation where the similarity of a frequent and a rare word is calculated, it will be lower than the similarity between two frequent words. Then the similarity judgment should not depend on the relative frequency of the words; instead, their tendency of agreement on features should take the dominant role.

For example, \NPMI/ makes ``feature selections'' by weighting PMI values with the co-occurrence frequency, as discussed in Section~\ref{sec:quantification-measures}. When cosine is applied, the stretching effect of \NPMI/ is eliminated, but the rotational effect stays. On average, the rotational effect will be much more significant for rare words, while frequent words are more likely to be stretched.

In addition to cosine and inner product, we use correlation \cite{kiela-clark:2014:CVSC} to measure similarity:
\begin{equation}
  \label{eq:correlation}
  \operatorname{correlation}(\vec{x}, \vec{y}) = \frac{(\vec{x} - \bar{x}) \cdot (\vec{y} - \bar{y})}
                                {\|(\vec{x} - \bar{x})\| \|(\vec{y}-\bar{y}\|}
\end{equation}
where $\bar{x}$ is the mean of the elements of $\vec{x}$ and $\bar{y}$ is the mean of the elements of $\vec{y}$.

\subsubsection{Compositional operator}
\label{sec:comp-oper}

\input{figures/comp-methods}

For phrasal tasks, the phrase vectors are obtained via composition of the phrase constituents' vectors using addition, multiplication \cite{mitchell2010composition,mitchell-lapata:2008:ACLMain}, Kronecker \cite{Grefenstette:2011:ESC:2145432.2145580} and tensor-based operators \cite{DBLP:journals/corr/abs-1003-4394,kartsadrqpl2014,kartsaklis-sadrzadeh-pulman:2012:POSTERS,Grefenstette:2011:ESC:2145432.2145580}. Table~\ref{tbl:comp-methods} lists the operators used in this study.

As a non-compositional baseline, we take the dummy operator \texttt{head}, which ignores the subject and the object of a phrase, causing the vector of a whole phrase to be equal to the vector of its verb.

\section{Hypotheses}
\label{sec:hypotheses}

To conduct the study, we introduce hypotheses that reflect the current state in the field of distributional semantics and facilitate the answering of the research questions.

\subsection{General}
\label{sec:general-hyp}

\begin{hyp}[H\ref{hyp:overfitting}]
\label{hyp:overfitting}
Heuristics-based model selection avoids overfitting.
\end{hyp}

We expect that models that are chosen using heuristics achieve better results on the datasets they were not instantiated on than the best models.

\begin{hyp}[H\ref{hyp:10percent}]
\label{hyp:10percent}
The relative difference between the score of the best model and the score of the model selected using heuristics is less than 10\%.
\end{hyp}

The optimal results reported in \newcite[\textcolor{citecolor}{Table~5}]{lapesa2014large} are within the 10\% margin (with an exception of the ESSLLI dataset, where the margin is 21\%). We expect similar relative differences in our results.

\begin{hyp}[H\ref{hyp:var}]
\label{hyp:var}
High-dimensional models are more likely to perform better than their low-dimensional counterparts.
\end{hyp}

As the vector space dimensionality increases, performance stabilises \cite{kiela-clark:2014:CVSC,BullinariaLevy2012,lapesa2014large}. We speculate that in a high-dimensional case the difference between parameter choices matters less and thus higher results are reported more often for high-dimensional spaces.

\begin{hyp}[H\ref{hyp:universal}]
\label{hyp:universal}
There is a universal model that performs well on a broad range of tasks.
\end{hyp}

We are interested to see whether there is one parameter choice that performs competitively on all tasks. The results of \citet{lapesa2014large} show that there is a general model whose performance is close to dataset- and task-specific models. We expect this to be the case also for compositional models.

\subsection{Parameter dependence on dimensionality}
\label{sec:hyp-dimen}

\begin{hyp}[H\ref{hyp:dimen}]
\label{hyp:dimen}
The optimal parameter choice depends on dimensionality.
\end{hyp}

We expect that the co-occurrence counts of the most frequent pairs do not contain noise (note that dimensions are ranked by the frequency of the corresponding features, so by design the low-dimensional vector spaces incorporate the most frequent features). The counts, and therefore the probability estimates, of less frequent pairs are noisy and require a special treatment to compensate for PMI's Achilles heel when small co-occurrence counts lead to extremely high PMI values \cite{TACL570}.

\begin{hyp}[H\ref{hyp:freq}]
  \label{hyp:freq}
 $N$ and $\log n$ frequency components are beneficial for high-dimensional spaces.
\end{hyp}

This is the most direct way of boosting high co-occurrence counts \cite{Evert05}. \NPMI/ is shown to be a good choice in lexical tasks \cite{Bruni:2012:DST:2390524.2390544}.

\begin{hyp}[H\ref{hyp:cds}]
  \label{hyp:cds}
  Low-dimensional spaces do not need context distribution smoothing, while high-dimensional spaces benefit from it.
\end{hyp}

This is because the estimated probabilities of rare contexts are noisy. This smoothing is shown successfully in high-dimensional count-based models \cite{TACL570} and word2vec \cite{mikolov2013efficient}. However, these recommendations were not tested on low-dimensional vector spaces, that are widely used by compositional models. 

\begin{hyp}[H\ref{hyp:neg}]
  \label{hyp:neg}
  Low-dimensional spaces benefit from being dense, while high-dimensional spaces benefit from being sparse.
\end{hyp}

Sparsity is controlled by the shifting parameter $k$; lower $k$ values make vectors denser.

\begin{hyp}[H\ref{hyp:similarity}]
  \label{hyp:similarity}
  Cosine is an optimal similarity measure for low-dimensional spaces, while correlation is for high-dimensional spaces.
\end{hyp}

Correlation is shown to be the best choice by \citet{kiela-clark:2014:CVSC} while cosine is generally perceived as the best similarity metric. It might be the case that the standardisation of vector values by subtracting the mean is effective for high-dimensional spaces.

\subsection{Lexical}
\label{sec:elab-hypoth-lexical}

\begin{hyp}[H\ref{hyp:lex-pmi-cpmi}]
  \label{hyp:lex-pmi-cpmi}
  In lexical tasks, there should be little difference between PMI and its compressed version CPMI.
% Shifted PMI variants behave the same (Section~\ref{sec:shifted-pmi}).
\end{hyp}

The main effect of CPMI is to transform negative values into the positive range of $(0; 1)$. One of the reasons to avoid negative values is that they might be problematic for multiplication during composition, as the sign of the result depends on the number of negative components. However, as there is no composition involved in lexical tasks, the weighting schemes should behave equally.

\subsection{Compositional}
\label{sec:hyp-composition}

\begin{hyp}[H\ref{hyp:not-lextocomp}]
\label{hyp:not-lextocomp}
Models that perform well on lexical tasks also perform well on compositional tasks.
\end{hyp}

If this is the case, then iterative tuning of parameters is justified. As an important consequence, the studies that perform evaluation of compositional models do not need to explore all possible parameter combinations.

\begin{hyp}[H\ref{hyp:order}]
\label{hyp:order}
The best models for compositional tasks should take word order into account.
\end{hyp}

We expect that the word order sensitive models outperform the models that ignore word order.

\begin{hyp}[H\ref{hyp:comp-pmi-cpmi}]
  \label{hyp:comp-pmi-cpmi}
  Compositional methods that include addition perform best with either PMI or SPMI, while methods that include multiplication should work best with CPMI or SCPMI.
\end{hyp}

One of the reasons for this is the presence of negative values. For example, in the case of multiplication as a compositional operator, the sign of a vector component depends on the number of the corresponding negative components of the constituents. If the number of negative values is odd, then the resulting value will be negative---this makes the difference between 0.001 and -0.001 significant. In the first case, the value means that the co-occurrence pair is weakly associated. But in the second case, the value means that the co-occurrence is weakly unassociated. This also applies to categorical operators where the signs of the result vector components depends on the number of multiplication operations that leads to them.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
