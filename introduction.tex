\chapter{Introduction}
\label{ch:introduction}

Computers, machines that play a more and more important role in our lives,
require specially designed programming languages to be controlled. This is
different from interactions between people where spoken or written natural
language is used. Ideally, it would be perfect if the interaction with computers
was not different to the interaction between humans. Computational linguistics is
one of the fields that aims to solve this problem.

In order to be controlled by people, or be able to assist people in language
related tasks, computers need to understand it. However, different tasks need
various level of language ``understanding''. For instance, even if one does not
recognize or know the language of a piece of text on Figure~\ref{fig:lv}, he or
she can tell how many words there are, and that there is only one
sentence. After a while, one can even say that this is probably a piece of
poetry.

The conclusions above require neither the complete understanding of the language nor the
meaning of the text. Knowledge that texts (at least in some languages) consist
of words separated by a space and how poems are usually written is
enough. Moreover, knowing the letter combination distribution across the
languages or a list of words for all human languages, one would conclude that
the text on Figure~\ref{fig:lv} is in Latvian. We managed to get this answers
without knowing what the text is about.

\input{poem.tex}

On the other hand, a task that asks to provide a list of associations with the text, an essay or a painting inspired by it demands a much better understanding of the text that requires the language knowledge and familiarity with the culture. Luckily, nowadays these kind of problems are not expected to be done by computers. People enjoy doing these kind of tasks themselves.

However, it is reasonable to ask a computer the following questions regarding text meaning:
\begin{inparaenum}[a)]
\item What is the text on Figure~\ref{fig:lv} about?
\item What is the relation between the texts on Figure~\ref{fig:lv} and
  Figure~\ref{fig:ru}?
\item Are these texts identical?
\item Where did the meeting took place?
\item What poems are similar to this?
\end{inparaenum}

Text summarizaiton, machine translation, information extraction and retrieval are just a few of many branches of computational linguistics that provide methods for answering these questions. The questions above have a general property: all of them are about the meaning of the text. Natural language semantics is an area that studies meaning representation and thus is necessary to solve the tasks.

Creativity of natural languages---the fact that humans are able to produce and understand sentences they have never came across---complicates meaning modeling. Even if we had a way to map each word to its meaning, it is impractical to apply the same procedure to sentences, because as we process a piece of text most of the sentences in it will be seen for the fist time. Because of this, we need to be able to build the meaning representation of a sentence from its constituent parts: words.

Syntax is a study about the structure of a sentence. Grammars define the rules that are describe how a sentences that belong to a language should look like. For example, a subject is in front of a verb and an object is after it in an English sentence. Having the constituent meaning representation, the meaning of a sentence is built guided by its syntax.

To a first, high level approximation, to be able to deal with the meaning of a text in natural language one needs to have meaning representation of constituents, a view to the (syntactic) structure of the text and a compositional procedure that outputs the meaning representation of the whole text.

Lexical distributional semantics \cite{BullinariaLevy2012,Bullinaria2007,Turney:2010:FMV:1861751.1861756} in recent years has been advanced after the success of the word2vec model \cite{mikolov2013linguistic,mikolov2013distributed,mikolov2013efficient} and practically redefined how the distributional lexical representations are obtained \cite{baroni-dinu-kruszewski:2014:P14-1}. On the other side, advances in compositional distributional semantics \cite{mitchell2010composition,maillard-clark-grefenstette:2014:TTNLS,Grefenstette:2011:ESC:2145432.2145580,Grefenstette:2011:ETV:2140490.2140497,kartsadrqpl2014,fried-polajnar-clark:2015:ACL-IJCNLP} showed that Frege's principle of compositionality \cite{Janssen2001} is useful in obtaining representations of phrases and sentences.

Even though the new generation of lexical vectors is shown to be superior than the old-fashioned ``counting'' models \cite{milajevs-EtAl:2014:EMNLP2014} in compositional tasks, this work adopts the recommendations of \newcite{TACL570} for model parameter tuning and by applying a method robust to overfitting based on \newcite{lapesa2014large} performs a large-scale model selection study.

The experiments show that after a careful selection of the vector space model parameters, the selected simple models are competitive with the current state of the art results and on some datasets even outperform it.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
