\chapter{Relationships between words}
\label{sec:lexical}

\lettrine[lines=5,loversize=0.25]{T}{his} chapter describes experiments to discover optimal parameters for lexical similarity, and the behavioural patterns of the parameters.\footnotemark{}
\footnotetext{Much of the work in this chapter has appeared as \newcite{milajevs-sadrzadeh-purver:2016:ACL-SRW} at the ACL Student Workshop 2016.} Simlex-999 \cite{hill2014simlex} and MEN \cite{Bruni:2014:MDS:2655713.2655714} are the two datasets that are used for evaluation. They provide averaged values of similarity judgements between pairs of words.

The experiment results and selected parameters are reported on the datasets individually. Then, the model selection is performed on a combination of the two datasets. The results are selected using three methods: Max selection, where the best score is selected, cross-validation, which separates model selection and score computation, and a selection based on heuristics, here parameters are chosen by their influence.

The experiments show that while the best parameter selection depends on the dataset, there is a global optimal parameter selection that is good on all datasets. In general, we find that non-constant frequency component, context distribution smoothing and shifting should be used for high-dimensional spaces to compensate for the noise that is introduced by a large number of context features.

\section{Experiments on SimLex-999 dataset}
\label{sec:simlex-999}

SimLex-999 is a dataset to evaluate lexical semantic models
\cite{hill2014simlex}. It tests how well a model captures similarity between
word pairs. The dataset implicitly distinguishes similarity and relatedness, so
related pairs such that \textit{coffee}, \textit{cup} are not considered to be
similar in this dataset. It consists of 666 noun-noun pairs, 222 verb-verb pairs
and 111 adjective-adjective pairs.  The models are evaluated by computing the
Spearman's-$\rho$ (the correlation of ranked model predictions with ranked human
judgements). Minimum required difference for significance (MRDS,
\citet{rastogi-vandurme-arora:2015:NAACL-HLT}) is used for statistical
significance testing. For SimLex-999, $\sigma^{0.9}_{0.05} = 0.023$.\footnotemark{}
\footnotetext{The results are available at \url{\BASEURL/results_all.csv}.}

\subsection{Max selection}
\label{sec:max-selection-simlex}

\input{figures/SimLex999-results}

Figure~\ref{fig:SimLex999-results} illustrates the results based on the best model selection and Table~\ref{tab:Simlex999-max-selection} shows the results together with chosen parameters. Note that maximum selection is identical with cross-validation: they pick the same models.

In general, model performance increases as dimensionality increases. There is no statistically significant difference between the scores with an
exception of the 5\,000 model, which underperforms. The best result of 0.389 is achieved with a 2\,000 dimensional space. Model performance becomes stable for dimensions greater than 20\,000, which suggests that rare context features contribute very little to similarity estimation.

\input{figures/SimLex999-max-selection-table}

As we see later in Section~\ref{sec:simlex-men} the best result is an example of
overfitting. The 2\,000 dimensional model performs very well on SimLex-999
achieving the score of 0.389, but underperforms on MEN achieving the score of
0.660, while the model chosen using heuristics yields the score of 0.724 on MEN
(the difference is statistically significant). In addition, the model selected
by using heuristics is not statistically significantly different from the maximum result for this dimensionality (0.728, Figure~\ref{fig:SimLex999-transfer}).

For spaces with dimensionality less than 5\,000, the frequency parameter set to 1 and the inner product as the similarity measure yield the best results. Otherwise, cosine with \logNSCPMI/, smoothing $\alpha=0.75$ and shifting $k=0.7$ gives the best results. This supports our hypotheses that high-dimensional spaces benefit from a non-constant frequency (H\ref{hyp:freq}), smoothing of context distribution (H\ref{hyp:cds}) and the sparsity of vectors (H\ref{hyp:neg}).

\subsection{Heuristics}
\label{sec:heuristics-simlex}

% \input{figures/SimLex999-interaction}

Analysis of variance is used to obtain parameter influence. First, we fit a linear regression model that takes into account all model parameters. The similarity score is a dependent variable, while the parameters of a similarity model are independent variables. In addition to individual parameter influence, we also consider their two-way interactions. The full linear model is defined as:
\begin{equation}
  \text{\it score} \sim \sum_{p \in P}p + \sum_{p, p' \in P \times P}p \cdot p'
\end{equation}
where $P$ is the set of parameters, in our case $P = \{\text{\it freq}, \text{\it discr}, \text{\it cds}, \text{\it neg}, \text{\it similarity} \}$.

The linear model achieves an adjusted $R^2$ value of 0.867, indicating that the linear model is able to predict the performance of a similarity model based on the parameters $P$ quite well.

After fitting a model on the full set of parameters,  we fit six ``restricted'' linear models, each of them excluding a parameter $\bar p$:
\begin{equation}
  \text{\it score} \sim \sum_{p \in P \setminus \{\bar p\} }p + \sum_{p, p' \in (P \setminus \{\bar p\}) \times (P \setminus \{\bar p\})}p \cdot p'
\end{equation}

\input{figures/SimLex999-heuristics-selection-table}

\input{figures/SimLex999-ablation}

We compute $R^2$ scores of the models that exclude a parameter. The difference of the $R^2$ sore of the full model and the $R^2$ score of a restricted model is the partial $R^2$ score of the excluded parameter. Table~\ref{tab:SimLex999-ablation} shows partial $R^2$ scores for all the parameters. The most influential parameters, in decreasing order, are similarity, \texttt{freq} and \texttt{neg}, the contribution of other parameters to model performance estimation is minimal.

\input{figures/SimLex999-similarity}
Figure~\ref{fig:SimLex999-similarity} shows the average performance of similarity measures. Correlation outperforms all other measures for all dimensions and peaks at the dimensionality  of 20\,000, as Table~\ref{tab:Simlex999-heuristics-selection} shows. However, for $D \leq 5\,000$, the difference between correlation-based and cosine-based similarity measures is less than for $D > 5\,000$, supporting H\ref{hyp:similarity} that correlation performs well with high-dimensional spaces.

The influence of \texttt{freq}, the second parameter, is shown in Figure~\ref{fig:SimLex999-freq}. $\log n$ frequency outperforms other choices for all dimensions. After 20\,000 dimensions, $\log n$'s performance stabilises: variance decreases and the performance stays constant. Taking into account that for $D \leq 5\,000$ there is little difference between $1$ and $\log n$, H\ref{hyp:freq} is supported in this case as well, suggesting that for low-dimensional spaces the frequency component is unnecessary.

The third parameter \texttt{neg}, with a value of 0.7, shows the best performance (Figure~\ref{fig:SimLex999-neg}). However, there is little difference between models with dimensionality greater than 20\,000. The score variance is much lower for high-dimensional spaces than for low-dimensional spaces, so high-dimensional spaces perform well with most choices of \texttt{neg}. This gives support to H\ref{hyp:var} that high-dimensional models are more likely to outperform low-dimensional models.
Also, lower $k$ values, which make vectors denser, lead to higher performance in low-dimensional spaces, supporting H\ref{hyp:neg} that expects low-dimensional models benefit from dense vectors.

\input{figures/SimLex999-cds}

Models that do not perform shifting (abbreviated as N/A in Figure~\ref{fig:SimLex999-neg}), peak at 20\,000 dimensions and decrease afterwards with increasing variance.

There is little difference between SPMI and SCPMI performance with a slight advantage to SCPMI (Figure~\ref{fig:SimLex999-discr}): PMI value compression into the range of $(0; 1)$ is unnecessary for lexical tasks (H\ref{hyp:lex-pmi-cpmi}).

Finally, models benefit from context distribution smoothing; spaces with less than 10\,000 dimensions produce the best results with $\alpha = 1$. For spaces with higher dimensionality, $\alpha = 0.75$ is the most advantageous (Figure~\ref{fig:SimLex999-cds}). This supports H\ref{hyp:cds} that smoothing is beneficial for high-dimensional spaces and corresponds to the preliminary results that were presented in \newcite{milajevs-sadrzadeh-purver:2016:ACL-SRW}.

\subsection{Difference between Max selection and heuristics on SimLex-999}

As expected, manual, heuristic-based parameter selection is more homogeneous, as Table~\ref{tab:Simlex999-heuristics-selection} shows. Both selection models agree on parameters for high-dimensional spaces ($D \geq 2\,000$), with an exception of similarity: Max selection prefers cosine, while manual prefers correlation-based similarity measures. Because of this, manual selection does not pick the best result for the 2\,000 dimensional model, but at 50\,000 dimensions a model selected manually scores only 0.001 lower: 0.384 versus 0.385 as also seen on Figure~\ref{fig:SimLex999-results}.

The average relative difference between Max selection and heuristics is 0.039 (3.9\%), which is within the 10\% margin set by H\ref{hyp:10percent}.

\section{Experiments on MEN dataset}
\label{sec:men}

\input{figures/men-results}

The MEN test collection consists of 3\,000 word pairs judged for similarity \cite{Bruni:2014:MDS:2655713.2655714}. In contrast to SimLex-999, the dataset does not distinguish between similarity and relatedness. As with SimLex-999, the models are evaluated by the Spearman's-$\rho$ correlation.

\subsection{Max selection}
\label{sec:max-selection-men}

\input{figures/men-max-selection-table}

Figure~\ref{fig:men-results} shows the selection results. Again, cross-validation results are identical with Max selection. Table~\ref{tab:men-max-selection} shows the results together with the selected models.

Model performance monotonically increases as dimensionality increases. The highest score of 0.765 is achieved by 3 spaces with $D \geq 30\,000$, \logNSCPMI/, smoothed context distribution ($\alpha = 0.75$), shifted PMI values ($k = 1$) and the similarity measure based on correlation.

In comparison with SimLex-999, models with ``more extreme'' parameters give better results on MEN. For example, $\alpha = 0.75$ is the best for models tested on SimLex-999 with dimensionality starting with 20\,000, while for models tested on MEN, this parameter choice is the best starting with 5\,000. Similar behaviour is observed for \texttt{neg} and similarity. For high-dimensional spaces, the switch from SimLex-999 to MEN changes the best \texttt{neg} choice from 0.7 to 1 and similarity from cosine to correlation. Such a difference in parameter choices might suggest the difference between \textit{relatedness} and \textit{similarity}, but it still supports H\ref{hyp:freq}, H\ref{hyp:cds} and H\ref{hyp:neg} that frequency, context distribution smoothing and sparsity are important for high-dimensional spaces. H\ref{hyp:similarity} is also supported, as correlation is the best similarity measure for high-dimensional spaces.

\subsection{Heuristics}
\label{sec:heuristics-men}

\input{figures/men-ablation}
The linear model gives an adjusted $R^2$ of 0.733, which is lower than on SimLex-999, but is still high. Table~\ref{tab:men-ablation} shows partial $R^2$ scores for the explored parameters. The most influential parameter is \texttt{neg}, followed by \texttt{freq} and similarity. This is different from the case of SimLex-999, where the parameter's influence ``order'' is reversed.

\input{figures/men-neg}
The parameter \texttt{neg}, which controls sparsity, with $k = 2$ is preferable for spaces with dimensionality less than 20\,000. For spaces with more dimensions,
$k = 5$ is more beneficial (Figure~\ref{fig:men-neg}). This replicates the suggestions of \newcite{TACL570}. We, however, expect that for spaces with more than 50\,000 dimensions even higher values should be preferred. This contrasts with the heuristics derived from SimLex-999, where a single \texttt{neg} value of 0.7 is chosen, but still complies with H\ref{hyp:neg}: sparse high-dimensional spaces outperform their high-dimensional, but dense counterparts.

Regarding the frequency component, $\log n$ outperforms all other choices (Figure~\ref{fig:men-freq}). It is exactly the same behaviour as for heuristics based on SimLex-999, including the fact that $1$ and $\log n$ behave similarly for $D \leq 50\,000$, so H\ref{hyp:freq}---that frequency is needed for high-dimensional spaces---is once again confirmed.

\input{figures/men-heuristics-selection-table}

\input{figures/men-similarity}
Correlation is the preferred similarity measure (Figure~\ref{fig:men-similarity}), which is, again, in line with the choice based on SimLex-999. However, the gap between cosine and correlation similarities stays constant, with an exception of $D = 1\,000$, where the gap is smaller, giving a weak support to H\ref{hyp:similarity}: correlation performs best with high-dimensional spaces.

SPMI is the preferred discriminativeness (Figure~\ref{fig:men-discr}), however, it is closely followed by CPMI and SCPMI. This contrasts with SimLex-999, where SCPMI is preferred. However, in both cases, the difference between the two choices is minimal. This is consistent with H\ref{hyp:lex-pmi-cpmi} that lexical models do not need PMI compression.

\input{figures/men-cds}
Global context probability gives on average higher results for MEN (Figure~\ref{fig:men-cds}). Note, SimLex-999 prefers context distribution smoothing (Figure~\ref{fig:SimLex999-cds}). The difference in performance between local context probabilities and global context probabilities decreases as dimensionality increases, making a weak support of H\ref{hyp:cds} that high-dimensional spaces benefit from context distribution smoothing.

\subsection{Difference between Max selection and heuristics on MEN}

The two selection procedures agree on fewer parameters than the ones based on SimLex-999. Both agree on discrimination ($\log n$) and similarity score for spaces with dimensionality greater than 10\,000 (correlation). While SCPMI is chosen by Max selection, SPMI is preferred by the selection based on heuristics, however, the difference between the two is minimal. In contrast to the Max selection, which chooses the models with context distribution smoothing, heuristics prefers models with global context probabilities. Also, heuristics picks models with higher shifting values $\alpha$ (2 and 5), in contrast to Max selection, where 0.7 and 1 are chosen. Table~\ref{tab:men-heuristics-selection} summarises the parameter selection based on heuristics.

The average difference between Max selection and heuristics is 0.008 (0.8\%), supporting H\ref{hyp:10percent}: the difference between Max selection and heuristics is within the 10\% margin.

\subsection{Difference between heuristics based on MEN and SimLex-999}

Heuristics based on MEN agree with ones based on SimLex-999 for two parameters: frequency ($\log n$) and similarity (correlation). The methods disagree on \texttt{discr} (SCPMI versus SPMI, respectively). However, the difference is negligible, because the compression of PMI values should not affect lexical similarity, as we expect by H\ref{hyp:lex-pmi-cpmi}. Context distribution (smoothed versus global) and shifting parameter (higher values of $k$ perform better on MEN) are other differences between the parameter selection based on the two datasets.

\section{Transfer of selected models between datasets}
\label{sec:select-model-transf}

We have identified parameters that lead to high correlation scores with human similarity judgements. However, we did not check whether the chosen models overfit. By transferring chosen models across datasets, that is taking parameters that are good on one dataset and applying them on another dataset, we should see whether chosen models overfit. 

\subsection{From SimLex-999 to MEN}
\label{sec:simlex-men}

\input{figures/lexical-transfer}

The models selected using heuristics based on the SimLex-999 dataset perform well on MEN: for all dimensions, the selected models are close to the best possible score (Figure~\ref{fig:SimLex999-transfer}). The average difference with the upper bound is 0.006, or 0.6\%.

The Max-based selection comes close to the upper bound for models with dimensionality greater than 5\,000. The average difference with the upper bound is 0.039. The higher difference is due to overfitting of the low-dimensional models ($D < 5\,000$), where the average difference is 0.09.

In this case, heuristic-based selection leads to better performance than the Max-based selection, supporting H\ref{hyp:overfitting}: Max selection does overfit.

\subsection{From MEN to SimLex-999}

Heuristics transferred from MEN to SimLex-999 behave less efficiently, they do not always outperform Max selection, though for high-dimensional spaces the difference decreases (Figure~\ref{fig:men-transfer}). The average difference is 0.062, which is ten times more than the transition from SimLex-999 to MEN.

Neither Max selection picks the best possible results when transferred from MEN to SimLex-999, however, the average difference is lower than with heuristics: 0.042 versus 0.062. This is similar to the transition in other direction.

Max-based selection leads to better performance than the heuristics for MEN, making a case against H\ref{hyp:overfitting}: Max selection does not overfit.

\section{Universal parameter selection for lexical datasets}
\label{sec:universal-lexical-param-selection}

\input{figures/lexical-results}

This section explores whether there are models that behave well across several datasets. It serves two interests. First of all, both datasets measure similarity, so the underlying method of estimating it ideally should not depend on the dataset. Secondly, by compromising between the two datasets, overfitting should be avoided.

We score the models based on the average of the normalised scores over SimLex-999 and MEN:
\begin{equation}
\operatorname{score}_\mathit{lexical}(\mathit{model}) =%
\frac{1}{2}\times%
\frac{\operatorname{score}_\mathit{SimLex-999}(\mathit{model})}%
{\max_m\operatorname{score}_\mathit{SimLex-999}(m)}%
+%
\frac{1}{2}\times%
\frac{\operatorname{score}_\mathit{MEN}(\mathit{model})}%
{\max_m\operatorname{score}_\mathit{MEN}(m)}%
\end{equation}

This is the simplest scoring methods. It treats the datasets equally by giving equal weights to them, without giving any preferences. We normalise the scores to compensate for various magnitudes of representative results between SimLex-999 and MEN.

The performance of the selected models on both datasets and the normalised average is shown in Table~\ref{tab:lexical-max-selection} (Max selection) and Table~\ref{tab:lexical-heuristics-selection} (selection based on heuristics) and in Figure~\ref{fig:lexical-results}.

\input{figures/lexical-max-selection-table}

\subsection{Max selection}
\label{sec:max-selection}

In general, the more dimensions, the better the results are. The selection yields the best results at $D = 50\,000$ for SimLex-999 and at $D = 30\,000$ for MEN. While for SimLex-999, the Max selection approaches the upper limit after 20\,000 dimensions; for MEN, the performance peaks at 30\,000 dimensions and then slightly deviates from the upper bound as the dimensionality increases.

The Max selection based on the combination of the two lexical datasets is closer to the Max selection based on SimLex-999 (Table~\ref{tab:Simlex999-max-selection}) than on MEN (Table~\ref{tab:men-max-selection}).

\input{figures/lexical-heuristics-selection-table}

\subsection{Heuristics}

The linear model achieves an adjusted $R^2$ of 0.817, which is less than the $R^2 = 0.867$ of SimLex-999, but is greater than the $R^2 = 0.733$ of MEN. Table~\ref{tab:lexical-ablation} shows partial $R^2$s for each parameter---the most influential are similarity, \texttt{neg} and \texttt{freq}.

\input{figures/lexical-similarity}
Correlation is the similarity measure of choice (Figure~\ref{fig:lexical-similarity}). However, the difference between cosine and correlation is minimal for $D \leq 5\,000$ supporting H\ref{hyp:similarity} that correlation is beneficial for high-dimensional spaces.

\input{figures/lexical-ablation}

For the models with dimensionality less than 20\,000, shifting should be used with $k = 1$, otherwise, $k = 2$ is preferred (Figure~\ref{fig:lexical-neg}). This supports H\ref{hyp:neg} that the more dimensions a model has the sparser it should be.

\input{figures/lexical-freq}
The frequency parameter $\log n$, on average, performs the best as the frequency component (Figure~\ref{fig:lexical-freq}). But for $D \leq 5\,000$, 1 also performs competitively, supporting H\ref{hyp:freq} that the frequency component is most useful for high-dimensional spaces.

SCPMI is the preferred discrimination component, but SPMI is very close to it (Figure~\ref{fig:lexical-discr}), backing up H\ref{hyp:lex-pmi-cpmi} that PMI value compression is not needed in lexical tasks.

\input{figures/lexical-cds}
Global context probabilities, on average, behave the best (Figure~\ref{fig:lexical-cds}). However, global context probabilities and local context probabilities with $\alpha = 1$ yield close results for $D > 10\,000$, giving support to H\ref{hyp:cds} that context distribution smoothing is needed in high-dimensional spaces.

\subsection{Comparison with single dataset based selections}

Both selection methods mostly agree on frequency ($\log n$) and discriminativeness (SCPMI).

Context probability distribution smoothing varies between the selection methods but follows the corresponding procedures based on MEN.

The Max-based selection for \texttt{neg} follows the Max selection on SimLex-999.

Even though the similarity choice is different between the Max-based and heuristic-based selections, it is consistent with SimLex-999 in both cases and with MEN for the heuristic-based selection.

For the Max-based selection, the average difference is 0.020 on SimLex-999 and 0.004 for MEN.

For the heuristics-based selection, the average difference is 0.048 for SimLex-999 and 0.010 for MEN, which is within the 10\% limit set by H\ref{hyp:10percent}.

Max selection behaves better than the heuristics-based selection on the average difference, but we cannot check how well these two selections behave on other lexical datasets. This is evidence against H\ref{hyp:overfitting} suggesting that testing on multiple datasets avoids overfitting and manual selection becomes too conservative.

Based on the experiments, \logNSCPMI/ with shifting close to 1 is the quantification of choice for the lexical tasks, however, more work needs to be done to find a robust choice for context distribution smoothing and similarity measure.

\section{Conclusion}
\label{sec:conclusion-lexical}

Lexical experiments give support to most of the stated hypotheses. The optimal parameter choice depends on dimensionality (H\ref{hyp:dimen}). In particular, non constant frequency component (H\ref{hyp:freq}), context distribution smoothing (H\ref{hyp:cds}) and shifting (H\ref{hyp:neg}) are recommended to be applied for spaces with $D \geq 10\,000$.

The switch at 10\,000 dimensions is a ``parameter sweet spot,'' as parameter choice is not significant at these points; the most representative example of this is the behaviour of \texttt{cds} on SimLex-999 (Figure~\ref{fig:SimLex999-cds}). After that point, performance either converges (supporting H\ref{hyp:var}), as in the case of \texttt{neg} on SimLex-999 (Figure~\ref{fig:SimLex999-neg}), or there is one dominant choice, as for \texttt{freq} on SimLex-999 (Figure~\ref{fig:SimLex999-freq}).

As expected, we did not see a significant influence of the ``compression'' of the PMI values (H\ref{hyp:lex-pmi-cpmi}).

We could not find supporting evidence for H\ref{hyp:overfitting}, as Max-selected models performed well on transfer and do not overfit. Both selection methods are within the 10\% difference margin to the highest result (H\ref{hyp:10percent}), suggesting that there indeed might be a universal vector space (H\ref{hyp:universal}).

\input{figures/lexical-comparison}
On lexical tasks, the best results among the selected models are 0.384 (SimLex-999) and 0.764 (MEN). On the similarity dataset, scores are 0.009 points behind the PPMI model of \newcite{TACL570}, however, on the relatedness dataset, 0.019 points above. Note the difference in dimensionality, source corpora and window size. Table~\ref{tab:lexical-comparison} shows the results of SVD, SGNS and GloVe-based vector spaces are given for comparison.

The next step is to test compositional models. Before experiments are presented in Chapter~\ref{sec:sentential}, a new dataset that provides relevance judgements of pairs of sentences is presented in Chapter~\ref{sec:phraserel}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% TeX-engine: xetex
%%% End:
