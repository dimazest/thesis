\chapter{Background: Similarity between words and phrases}
\label{cha:background}

\section{The notion of similarity}
\label{sec:similarity}

\lettrine[lines=5,loversize=0.25]{S}{imilarity} is the degree of resemblance between two objects or events \cite{WCS:WCS1282}. It plays a crucial role in psychological theories of knowledge and behaviour, where similarity is used to explain such phenomena as classification and conceptualisation \cite{Tversky1977,1986-13502-00119860101,medin1993respects,Markman1996,hahn1997concepts}.

\textit{Fruit} is a \emph{category} because it is a practical generalisation. Fruits are sweet and generally are desserts, so when one is presented with an unseen fruit, one can hypothesise that it is served toward the end of a dinner.

Generalisations are extremely powerful in describing a language, as well. The verb \textit{runs} requires its subject to be singular. \textit{Verb}, \textit{subject} and \textit{singular} are categories that are used to describe English grammar. When one encounters an unknown word and is told that it is a verb, one will immediately have an idea about how to use it, assuming that it is used similarly to other English verbs.

From a computational perspective, this motivates and guides the development of similarity components that are embedded into automatic systems that deal with natural language.

The information that the word \textit{carpet} is similar in meaning to the word \textit{mat} might be exploited by a language model in estimating the probability of the sentence \textit{the cat sat on the carpet}, even if it did not occur in the corpus but \textit{the cat sat on the mat} did \cite{bengio2006}.

In Information Retrieval (IR), queries are expanded with related terms to increase the number of retrieved relevant documents. For example, if a user issues the query \textit{lakes in Sweden}, the system might add related words to the query such as \textit{lake}, \textit{reservoir}, \textit{river} or even \textit{swim} so that documents that do not contain the word \textit{lakes} are retrieved \cite{Xu:1996:QEU:243199.243202}.
% \cite{Salton:1975:VSM:361219.361220}

A dependency parser might benefit from a generalisation about the part-of-speech tag of a word which did not occur in the training data, based on its occurrence pattern in a large corpus of documents from the web \cite{hermann-blunsom:2013:ACL2013,andreas-klein:2014:P14-2}.

A dialogue act tagging system might require classification of an utterance based on its role in a dialogue, such as a question or an acknowledgement \cite{kalchbrenner-blunsom:2013:CVSC}.

The examples show that similarity is a broad term that is task-dependent. An IR system needs to identify semantically similar (\textit{lake}, \textit{river}) and related (\textit{lake}, \textit{swim}) terms. A dependency parser benefits from the similarity of word usage. A language model exploits similarity in word meaning. A dialogue act tagging system relies on the similarity of the roles that the utterances play in discourse.

A computational model that estimates similarity need not only take into account the different flavours of the similarity relation, but also be able to measure similarity between pairs of words, between pairs of phrases or even between whole sentences, utterances or documents.

\section{Representation for similarity measurement}
\label{sec:word-meaning}

According to \newcite{WCS:WCS1282}, ``similarity is an essentially psychological notion, based on the way we represent objects, that is, the way they appear to us.'' Since it is not yet known how objects are represented in the human mind, the computational way of object representation for similarity estimation has to be agreed upon. However, one needs to be extremely careful when the object representation is decided, as it is unavoidably connected to the \emph{meaning of words in isolation}.

% The semantic formalisation of similarity is based on two ideas. The occurrence pattern of a word \emph{defines} its meaning \cite{firth1957lingtheory}, while the difference in occurrence between two words \emph{quantifies} the difference in their meaning \cite{harris1954distributional}.

Frege discusses two conflicting principles of meaning \cite{Janssen2001}. According to \emph{the principle of compositionality}, isolated word meanings are the building blocks of sentence meanings:
\begin{displayquote}[\cite{Janssen2001}]
The meaning of a compound expression is a function of the meaning of its parts and the syntactic rule by which they are combined.
\end{displayquote}
%
However, according to \emph{the principle of contextuality}, the word meaning in isolation is not defined:
\begin{displayquote}[\cite{Janssen2001}]
Never ask for the meaning of a word in isolation, but only in the context of a sentence.
\end{displayquote}

It is worth noting here that the measurement of similarity in isolation is also problematic because the number of features an entity has is infinite, and it is easy to show that two entities will always have an infinite number of common features, making the degree of resemblance undefined \cite{goodman1972problems,hahn1997concepts}. For example, the tree next to my house is similar to my house because both of them are less than one kilometre in height; both of them are less than two kilometres in height; both of them are less than three kilometre in height; and so on.

To make similarity measurements possible, they have to be measured \emph{under a given description} \cite{WCS:WCS1282,medin1993respects,Markman1996}. Thus, similarity is always contextualised. In other words, similarity emerges only when the possible properties are weighted. In our example, the tree and the house are similar with respect to the colour: both of them are green. The height properties and other irrelevant properties are assigned zero weight, making the number of non-zero feature values finite.

Frege's principle of contextuality allows us to define the meaning of a word by identifying its contribution to the meaning of a sentence. Firth's \citeyearpar{firth1957lingtheory} famous quote that ``you shall know a word by the company it keeps,'' suggests that the word meaning can be \emph{modelled} as the combination of the meanings of its occurrences in sentences of a corpus. Note that this does not provide the absolute word meaning, but only its meaning relative to the corpus. This assumption is also supported by the distributional hypothesis of \newcite{harris1954distributional} that the differences of occurrences of two words quantify the difference in their relative meaning, but do not necessarily define the meaning.

Once the relative word meaning is accepted, compositionality can be used to obtain representations of phrases and sentences \cite{THEO:THEO373,Dowty1980,sep-montague-semantics,DBLP:journals/corr/abs-1003-4394,baroni2014frege}.

We continue with an overview of the main ideas behind the word and phrase representations for similarity measurement. It is later followed by the discussion of the common empirical evaluation procedures and the strategies of obtaining reliable evaluation results.

\section{Representation of words}
\label{sec:distr-hypoth}

In principle, we would like to capture the intuition that while \textit{John} and \textit{Mary} are distinct entities, they are rather similar to each other---because both of them are humans---and are dissimilar to \textit{dog}, \textit{pavement} or \textit{idea}. However, we start with the logical word meaning representation that captures the fact that entities are distinct but does not provide the means to measure similarity.

\subsection{Logical representations for inference}
\label{sec:classical-approaches}

Formal semantics provides the means to infer\footnotemark{} some piece of information from another. The main studied relation is the entailment of sentences, for example, \textit{John swims in Åresjön} entails \textit{John swims in a Swedish lake}. To evaluate entailment, the sentences are converted to formulas. The words correspond to symbols in formal logic.

\footnotetext{Work on natural logic demonstrates that it is not necessary to convert to a logical representations, see, for example, \citet{MacCartney:2007:NLT:1654536.1654575}.}

The individual word \textit{Åresjön} corresponds to the symbol \textit{Åresjön'}, which is mapped to the actual lake by the interpretation function $\mathcal{I}$.

One-place properties are seen as sets of individuals, so $\mathcal{I}(\mathit{Swedish'})$ is a set that contains $\mathcal{I}(\mathit{Åresjön'})$ and $\mathcal{I}(\mathit{Väsman'})$, among many other Swedish entities.

\textit{Swim'} is a two-place predicate that is represented as a set that contains the pairs between which the relation holds; so if John actually swims in Åresjön, then $\mathcal{I}(\mathit{swim'})$ will contain the pair $(\mathcal{I}(\mathit{John'}), \mathcal{I}(\mathit{Åresjön'}))$.

While such formalism is very powerful for entailment detection between sentences, similarity measurement is problematic,\footnotemark{} because there is no relation between atomic symbols: we only know that \textit{Åresjön} and \textit{Väsman} correspond to different entities in the universe, but know nothing about the resemblance between their properties.

\footnotetext{Such property could be perfectly expressed in formal semantics, but it is not generally seen as part of the job of formal semantics, also acquiring such properties is hard in comparison to the distributional methods.}

\subsection{Distributional representations for similarity}
\label{sec:distributional-representations}

Distributional methods provide a way to measure the similarity between words. The representations are produced by exploiting Harris' \citeyearpar{harris1954distributional} intuition that similar words occur in similar contexts.

% \subsubsection{Distributional semantics: global representations}
\label{sec:distr-repr}

\begin{wraptable}[10]{O}{7cm}
  \centering
  %\vspace{-1em}
  \begin{tabular}{lrrr}
    \toprule
    & philosophy & book & school \\
    \midrule
    John & 4  & 60 & 59  \\
    Mary & 0  & 10 & 22  \\
    girl & 0  & 19 & 93  \\
    boy  & 0  & 12 & 146 \\
    idea & 10 & 47 & 39  \\
    \bottomrule
  \end{tabular}
  \caption{Word co-occurrence frequencies extracted from the BNC}
  \label{tab:comparison}
\end{wraptable}

A common approach is to construct a vector space in which the dimensions correspond to contexts, which are usually other words \cite{Turney:2010:FMV:1861751.1861756}. The components of the vector of a word can be calculated by taking the frequency with which the word co-occurred with the corresponding contexts within a predefined window in a corpus of interest. The similarity in meaning can be expressed via a suitable distance metric within the space.

Table~\ref{tab:comparison} shows five three-dimensional vectors for words \textit{Mary}, \textit{John}, \textit{girl}, \textit{boy} and \textit{idea}. These are \textit{target words}.  The words \textit{philosophy}, \textit{book} and \textit{school} label vector space dimensions and are referred to as \emph{context words}. Table~\ref{tab:comparison} represents the global co-occurrence statistics, giving the name to the representations.

As the vector for \textit{Mary} is closer to \textit{girl} than it is to \textit{boy} in the vector space, we can say that \textit{Mary} shares more features with \textit{girl} (and less  with \textit{boy}), therefore \textit{Mary} is semantically more similar to \textit{girl} than to \textit{boy}.

Mathematically, the similarity can be expressed using, for instance, the cosine of the angle between two vectors:
%
\begin{align*}
\cos(\theta) &=
\frac{\ov{\mathit{Mary}}\cdot\ov{\mathit{girl}}}
{||\ov{\mathit{Mary}}||||\ov{\mathit{girl}}||} =
%
\frac{(0\times0) + (10\times19) + (22\times93)}
{\sqrt{0^2 + 10^2 + 22^2}\sqrt{0^2 + 19^2 + 93^2}} \approx
\frac{2236}{2294} \approx 0.975
 \\
\cos(\phi) &=
\frac{\ov{\mathit{Mary}}\cdot\ov{\mathit{boy}}}
{||\ov{\mathit{Mary}}||||\ov{\mathit{boy}}||} =
%
\frac{(0\times0) + (10\times12) + (22\times146)}
{\sqrt{0^2 + 10^2 + 22^2}\sqrt{0^2 + 12^2 + 146^2}} \approx
\frac{3332}{3540} \approx 0.941
\end{align*}
%
where $\theta$ is the angle between the vectors of \textit{Mary} and \textit{girl}; and $\phi$ is the angle between the vectors of \textit{Mary} and \textit{boy}.

In the current example of a na{\"\i}ve vector space, \textit{John} is also closer to \textit{girl} than to \textit{boy}, which is counter-intuitive. This might be because of the small number of dimensions used, the poor selection of the context words, or the usage of raw co-occurrence numbers.% The importance of choosing good parameters is discussed in details in Section~\ref{sec:parameter-selection-intro}.

% \subsubsection{Word embeddings: local representations}
% \label{sec:neural-embedding}

% Deep learning\footnotemark{} techniques use the distributional hypothesis differently than the distributional methods described above \cite{Goldberg2016}. Instead of relying on the co-occurrence matrix, the training process of the word2vec framework \cite{mikolov2013linguistic,mikolov2013distributed,mikolov2013efficient} iterates directly over the target-context pairs. At any given moment during the training process, it has access only to local co-occurrence information, the global table of co-occurrences is not used, thus the name of the method. The training is done by maximising an objective function based on the following:
% \footnotetext{
%   Even though the model of \newcite{mikolov2013linguistic,mikolov2013distributed,mikolov2013efficient} is not a deep neural network, its efficient implementation uses methods developed for deep models. For example, hierarchical softmax and noise contrastive estimation.
% }
% %
% \begin{equation}
%  \frac{1}{T}\sum^{T}_{t=1}\sum_{-c \leq j \leq c, j\neq0} \log P(w_{t+j}|w_t)
%   \label{eq:objective-func}
% \end{equation}
% %
% In practice, maximising this function produces vectors which maximise the conditional probability of observing words in a context around the target word $w_t$. $C$ is the size of the training context, and $w_1 w_2, \ldots, w_T$ is a sequence of training words. Empirically, they capture the distributional intuition and express degrees of lexical similarity. The probability of a context word $w_c$ given a target word $w_t$ is defined using the softmax function:
% %
% \begin{equation}
%   P(w_c|w_{t}) = \frac{e^{\vv{w_c} \cdot \vv{w_t}}}{\sum_{c'}e^{\vv{w_{c'}} \cdot \vv{w_t}}}
% \end{equation}
% %
% The word vectors are dense (their dimensionality is few hundreds). Because the objective in \eqref{eq:objective-func} is expensive to compute, negative sampling is used in practice, however its description is beyond the scope of this work.

% The objective function in \eqref{eq:objective-func} is used by the skip-gram model: for every target-context pair, the objective function is based on predicting context words given a target word. The prediction can be reversed, that is, given the context words the target word of a context is predicted. Formally this objective is expressed in \citet{mikolov2013efficient} giving rise to the continuous bag-of-words model (CBOW):
% \begin{equation}
%  \frac{1}{T}\sum^{T}_{t=1}\sum_{-c \leq j \leq c, j\neq0} \log P(w_{t}|w_{t+j})
%   \label{eq:objective-func-cbow}
% \end{equation}
% Here, the representation of the context is obtained by projecting several vectors of context words into one vector using addition.

\section{Representation of phrases and sentences}
\label{sec:similarity-compounds}

Both local and global distributional approaches have advantages over the formal approach in their ability to capture lexical semantics and degrees of similarity. However, their success at extending this to the sentence level, and to more complex semantic phenomena, depends on their applicability within compositional models.

\subsection{The principle of compositionality}
\label{sec:formal-semantics}

Formal approaches to the semantics of natural language have built upon the classical idea of compositionality which states that the meaning of a sentence is a function of its parts \cite{Janssen2001}. Syntactic rules define how constituents are recursively combined to form other constituents until the whole sentence is covered. Translation rules define how semantic representations of the constituents are combined to get a semantic representation of the whole.

In compositional type-logical approaches, predicate-argument structures representing phrases and sentences are built from their constituent parts by general operations such as beta-reduction within the lambda calculus \cite{THEO:THEO373}: for example, given a semantic representation of \emph{John}
as $\mathit{John}'$, \emph{loves} as $\lambda y.\lambda x.\mathit{loves}'(x, y)$ and \emph{Mary} as $\mathit{Mary'}$, the semantic representation of the sentence \emph{John loves Mary} can be constructed as
%
\begin{equation*}
\lambda y.\lambda
x.\mathit{loves}'(x, y)(\mathit{mary}')(\mathit{john}') =
\mathit{loves}'(\mathit{john}', \mathit{mary}')
\end{equation*}
%
Categorical grammars are widely used to obtain the syntactic structure of a sentence. Given a set of basic categories $\texttt{ATOM}$, for example $\{\mathit{n}, \mathit{s}, \mathit{np}\}$, complex categories $\mathtt{CAT} \backslash \mathtt{CAT}$ and $\mathtt{CAT}/\mathtt{CAT}$ can be constructed, where $\mathtt{CAT}$ is either an element of $\texttt{ATOM}$ or a complex category. So the category of a transitive verb is $\mathtt{np}\backslash\mathtt{s}/\mathtt{np}$. Intuitively, we want to express that to obtain a sentence with a transitive verb there must be two noun phrases, one before and another after the verb.

Parsing is done by composing categories together according to two rules:
%
\begin{enumerate}
\item \textbf{Backward application}: If $\alpha$ is a string of category $A$ and
  $\beta$ is a string of category $A\backslash{}B$, then $\alpha\beta$ is of
  category $B$.
\item \textbf{Forward application}: If $\alpha$ is a string of category $A$ and
  $\beta$ is a string of category $B/A$, then $\beta\alpha$ is of category $B$.
\end{enumerate}

\begin{figure}
  \centering
  \Tree [
    .$s$
    [
      .$\mathit{np}$
      John
    ]
    [
      .$\mathit{np}\backslash{}s$
      [
        .$\mathit{np}\backslash{}\mathit{s}/\mathit{np}$
        loves
      ]
      [
        .$\mathit{np}$
        Mary
      ]
    ]
  ]
  \caption[A syntactic tree]{A syntactic tree for \textit{John loves Mary}. The lexicon assigns
    categories to words: \textit{John} is $\mathit{np}$, loves is
    $\mathit{np}\backslash{}\mathit{s}/\mathit{np}$ and Mary is
    $\mathit{np}$. Backward and forward composition rules derive the syntactic
    tree.}
\label{fig:cg}
\end{figure}

Figure~\ref{fig:cg} illustrates the parse tree for \textit{John loves Mary}
obtained using the category composition rules.

The last step is to map syntactic categories with semantic terms. Again, there
are base types ($e$ for entities and $t$ for sentences) and complex types of the
form $(a \to b)$ where $a$ and $b$ are types. The mapping between syntactic
categories and semantic types is defined as a function $\mathit{type}$:
%
\begin{align*}
  &\mathit{type}(np) = e \\
  &\mathit{type}(s) = t \\
  &\mathit{type}(A/B) = (\mathit{type}(B) \to \mathit{type}(A)) \\
  &\mathit{type}(B\backslash{}A) = (\mathit{type}(B) \to \mathit{type}(A))
\end{align*}
%
Syntactic backward and forward application corresponds to functional
application. Figure~\ref{fig:syn} shows the final parse tree.

\begin{figure}
  \centering
  \Tree [
    .$s$~:~$\mathit{loves}'(\mathit{john}',\mathit{mary}')$
    [
      .$\mathit{np}$~:~$\mathit{john}'$
      John
    ]
    [
      .$\mathit{np}\backslash{}s$~:~$\lambda~x.\mathit{loves}'(x,~\mathit{mary}')$
      [
        .$\mathit{np}\backslash{}\mathit{s}/\mathit{np}$~:~$\lambda{}y.\lambda{}x.\mathit{loves}'(x,y)$
        loves
      ]
      [
        .$\mathit{np}$~:~$\mathit{mary}'$
        Mary
      ]
    ]
  ]
  \caption{The final parse tree}
\label{fig:syn}
\end{figure}

Given a suitable pairing between a syntactic grammar, semantic representations and corresponding general combinatory operators, this can produce structured sentential representations with a broad coverage and good generalisability \cite{step2008:2222}. This logical approach is extremely powerful because it can capture complex aspects of meaning such as quantifiers and their interactions \cite{Copestake2005}, and enables inference using well studied and developed logical methods \cite{bos2000first}.

\subsection{Compositional distributional semantics}
\label{sec:composition}

Methods based on the distributional hypothesis have been recently applied to many tasks, but mostly at the word level, for instance, word sense disambiguation \cite{ZhitomirskyGeffet2009} and lexical substitution \cite{Thater:2010:CSR:1858681.1858778}. They exploit the notion of similarity which correlates with the angle between word vectors \cite{Turney:2010:FMV:1861751.1861756}.

\emph{Compositional} distributional semantics goes beyond word level and models the meaning of phrases or sentences based on their parts. \newcite{mitchell-lapata:2008:ACLMain} perform composition of word vectors using vector addition and multiplication operations. The limitation of this approach is the operator commutativity, which ignores the argument order, and thus the syntactic/semantic function of the words (as a consequence for English, the word order is ignored). As a result, \textit{John loves Mary} and \textit{Mary loves John} get identical representations.

\subsubsection{Notation}

Before introducing the compositional operators, the used notation is introduced here.

Vectors of words are written as, for example, $\ov{\textit{mary}} = [0, 10, 22]$. The arrow above a word indicates that the representation of it is a vector. The vector values are either raw co-occurrence counts obtained from a corpus, as in the examples below, or their quantified counterparts.

Point-wise addition, written as $+$, and point-wise multiplication, written as $\odot$, are the two basic compositional operators. As the output of these operators is a vector, the vector that represents a sentence is written as the sentence itself with an arrow on top and the operator used in the bottom right: for example, $\ov{\text{\it John loves Mary}}_{\text{addition}}$ and $\ov{\text{\it John loves Mary}}_{\text{multiplication}}$ for addition and multiplication respectively.

Other operators require a verb to be represented as a matrix. The matrix can be obtained in various ways, in this work two types are used.

The Kronecker compositional operator (see below) uses the Kronecker product, written as $\otimes$, which is a generalization of the outer product:
%
\begin{equation}
  \label{eq:kron-def}
 \mathbf{A}\otimes\mathbf{B} = \begin{bmatrix} a_{11} \mathbf{B} & \cdots & a_{1n}\mathbf{B} \\ \vdots & \ddots & \vdots \\ a_{m1} \mathbf{B} & \cdots & a_{mn} \mathbf{B} \end{bmatrix} 
\end{equation}
where $\mathbf{A}$ is a $m \times n$ matrix and $\mathbf{B}$ is $p \times q$ matrix. In this work, vectors are used to produce verb matrices, making the actual computation simpler (for the vectors $\ov{a}$ and $\ov{b}$ of length $n$):
\begin{equation}
  \label{eq:outer}
  \ov{a} \otimes \ov{b} = \begin{bmatrix} a_1b_1 & \cdots & a_1b_n\\ \vdots & \ddots & \vdots \\ a_nb_1 & \cdots & a_nb_n\end{bmatrix}
\end{equation}

Verb matrices used by the Kronecker compositional operator are written as $\widetilde{\mathit{Verb}} = \ov{Verb} \otimes \ov{Verb}$. The tilde indicates that it is a matrix that, in turn, is the Kronecker product of the verb vector with itself.

As the output of the Kronecker compositional operator is a matrix, the representation of a sentence obtained using it is written with a line instead of an arrow, for instance, $\overline{\text{\it John loves Mary}}_{\text{Kronecker}}$, the operator is identified in the bottom right.

The second kind of verb matrices is written as $\overline{Verb}$ and obtained from a corpus by considering the Kronecker products of the subject-object pairs that occur with the verb, see below.

Finally, $^{\mathsf{T}}$ transposes a matrix. The transpose of the matrix $\mathbf{A}$ is written as $\mathbf{A}^{\mathsf{T}}$ and $\mathbf{A}_{ij} = \mathbf{A}^{\mathsf{T}}_{ji}$.

%\overline{\text{\it loves}}^{\mathsf{T}}

\subsubsection{Compositional operators}

Consider that \textit{John}, \textit{Mary} and \textit{loves} are represented as vectors $\ov{\text{\it john}} = [4, 60, 59]$, $\ov{\text{\it mary}} = [0, 10, 22]$ and $\ov{\text{\it loves}} = [10, 100, 20]$, respectively. Then the vector of the sentence \textit{John loves Mary} using addition is:
%
\begin{align*}
  \ov{\text{\it John loves Mary}}_{\text{addition}} &= \vv{\mathit{john}} + \ov{\mathit{loves}} + \ov{\mathit{mary}} \\
                                  &= [4, 60, 59] + [10, 100, 20] + [0, 10, 22] \\
                                  &= [14, 170, 101]
\end{align*}
%
It is similar for multiplication, where element-wise multiplication is used instead of addition:
%
\begin{align*}
  \ov{\text{\it John loves Mary}}_{\text{multiplication}} &= \ov{\mathit{john}} \odot \ov{\mathit{loves}} \odot \ov{\mathit{mary}} \\
                                  &= [4, 60, 59] \odot [10, 100, 20] \odot [0, 10, 22] \\
                                  &= [0, 60\,000, 25\,960]
\end{align*}
%
To capture word order and the syntactic structure of a sentence, various approaches have been proposed. \newcite{Grefenstette:2011:ESC:2145432.2145580} use non-commutative linear algebra operators as proposed in the theoretical work of \newcite{DBLP:journals/corr/abs-1003-4394}. There, the functional applications of semantic terms are replaced with tensors \cite{Bourbaki1998commutative}.

Kronecker \cite{Grefenstette:2011:ETV:2140490.2140497} is a non-commutative operator. It represents the verb of a phrase as a matrix and the subject and the object as vectors. The verb matrix is defined as the Kronecker product---which gives the name to the compositional operator---of the vector of a verb with itself:
%
\begin{equation}
  \widetilde{\mathit{Verb}} = \ov{\mathit{Verb}} \otimes \ov{\mathit{Verb}}
\end{equation}

The representation\footnotemark{} of a subject-verb-object phrase is computed as:
%
\begin{equation}
  \overline{\text{\it Sbj Verb Obj}} = \widetilde{\text{\it Verb}} \odot (\ov{\text{\it Sbj}} \otimes \ov{\text{\it Obj}})
\end{equation}
%
The derivation of the representation of \textit{John loves Mary} using Kronecker is:
\begin{align*}
  \overline{\text{\it John loves Mary}}_{\text{Kronecker}} &= \widetilde{\text{\it loves}} \odot (\ov{\text{\it john}} \otimes \ov{\text{\it mary}}) \\
                                  &= (\ov{\text{\it loves}} \otimes \ov{\text{\it loves}}) \odot (\ov{\text{\it john}} \otimes \ov{\text{\it mary}}) \\
                                  &=([10, 100, 20] \otimes [10, 100, 20]) \odot ([4, 60, 59] \otimes [0, 10, 22]) \\
  \displaybreak[0]
                                  &=\begin{bmatrix}
                                       100 &  1\,000 &     200 \\
                                    1\,000 & 10\,000 &  2\,000 \\
                                       200 &  2\,000 &     400
                                     \end{bmatrix} %
                                     \odot %
                                     \begin{bmatrix} %
                                         0 &     40 &      88  \\
                                         0 &    600 &  1\,320  \\
                                         0 &    590 &  1\,298
                                       \end{bmatrix} \\
  \displaybreak[0]
                                  &= \begin{bmatrix}
                                         0 &     40\,000 &     17\,600 \\
                                         0 & 6\,000\,000 & 2\,640\,000 \\
                                         0 & 1\,180\,000 &    519\,200
                                    \end{bmatrix}
\end{align*}

\footnotetext{The operators that include Kronecker product produce matrices, not vectors, however computing similarity between matrices is possible by collapsing the rows of the matrix into one dimension by concatenating them together.}

For other non-commutative operators presented below, a transitive verb is represented by the matrix $\overline{\text{\it Verb}}$, which is obtained from a corpus using the formula \cite{Grefenstette:2011:ESC:2145432.2145580}:
\begin{equation}
  \overline{\text{\it Verb}} = \sum_i\ov{s_i} \otimes \ov{o_i}
  \label{eq:verb-matrix}
\end{equation}
where $\ov{s_i}$ and $\ov{o_i}$ are the subject-object pairs of the verb found in the source corpus.

This encodes into the verb matrix all the contextual information of the subjects and objects of the verb. The way it is constructed (by pairing subjects and objects of the verb) is the same way a verb predicate is built in formal semantics, that is by pairing the subjects and objects. But here, instead of packing the pairs into a set, one develops a matrix from them, at the same time, each pair gets a real number value assigned to it.

To continue the example, imagine that the verb \textit{loves} was seen three times in the corpus: \textit{John loves Mary}, \textit{Peter likes coffee} and \textit{Mary likes hiking}. The matrix $\overline{\text{\it loves}}$ is computed as:
\begin{align*}
  \overline{\text{\it loves}} &= (\ov{\text{\it john}} \otimes \ov{\text{\it mary}}) + %
                                 (\ov{\text{\it peter}} \otimes \ov{\text{\it coffee}}) + %
                                 (\ov{\text{\it mary}} \otimes \ov{\text{\it hiking}}) = %
                                \begin{bmatrix}
                                     120 &  1\,390 &    658 \\
                                  1\,360 & 13\,670 & 2\,874 \\
                                     390 &  3\,930 &    846
                                \end{bmatrix}
\end{align*}

Relational compositional operator is defined as \cite{Grefenstette:2011:ESC:2145432.2145580}:
\begin{equation}
  \overline{\text{\it Verb}} \odot (\ov{\text{\it Sbj}} \otimes \ov{\text{\it Obj}})
\end{equation}

Informally, it identifies the interaction of the features of the subject and the object by the Kronecker product $\ov{\text{\it Sbj}} \otimes \ov{\text{\it Obj}}$ that are later filtered by verb using point-wise multiplication \cite{Grefenstette:2011:ESC:2145432.2145580}. The filtering by verb is needed because the subject and the object can belong to several relations.

In the example, the meaning is computed similarly to Kronecker, but a different verb matrix is used:
%
\begin{align*}
  \overline{\text{\it John loves Mary}}_{\text{relational}} &= \overline{\text{\it loves}} \odot (\ov{\text{\it john}} \otimes \ov{\text{\it mary}}) \\
  \displaybreak[0]
                                  &= \begin{bmatrix}
                                       120 &  1\,390 &    658 \\
                                    1\,360 & 13\,670 & 2\,874 \\
                                       390 &  3\,930 &    846
                                     \end{bmatrix} %
                                     \odot %
                                     \begin{bmatrix} %
                                         0 &     40 &      88  \\
                                         0 &    600 &  1\,320  \\
                                         0 &    590 &  1\,298
                                       \end{bmatrix} \\
  \displaybreak[0]
                                  &= \begin{bmatrix}
                                    0 &     55\,600 &     57\,904 \\
                                    0 & 8\,202\,000 & 3\,793\,680 \\
                                    0 & 2\,318\,700 & 1\,098\,108 \\
                                    \end{bmatrix}
\end{align*}

The categorical framework of \newcite{DBLP:journals/corr/abs-1003-4394} requires the meaning of a transitive verb to be a tensor of third order (it must be a cube), but the relational method of \newcite{Grefenstette:2011:ESC:2145432.2145580} constructs verb meanings as tensors of second order (they are matrices). To apply the categorical framework, \newcite{kartsaklis-sadrzadeh-pulman:2012:POSTERS} embed the second order verb matrix into a third order space by a map $\sigma: N^2 \to N^3$. This map is one of the maps of the Frobenius algebra over vector space N.

The Frobenius operation $\sigma$ turns a matrix into a cube by filling the extra dimension of the cube and gives some options on how the mapping can be implemented. The operators copy-subject and copy-object \cite{kartsaklis-sadrzadeh-pulman:2012:POSTERS} implement this idea. The cube is formed using copy-subject (copy-object), when one applies cube contraction on it with the object (the subject) and then matrix multiplication with the subject (the object).

Copy-object diagonally places the object dimension into a cube, producing a closed form-formula:
%
\begin{equation}
  \ov{\text{\it Sbj}} \odot (\overline{\text{\it Verb}} \times \ov{\text{\it Obj}})
\end{equation}
%
where $\times$ is matrix multiplication. The vector representation of \textit{John loves Mary} using copy-object is:
%
\begin{align*}
  \overline{\text{\it John loves Mary}}_{\text{copy-object}} &=\ov{\text{\it john}} \odot (\overline{\text{\it loves}} \times \ov{\text{\it mary}}) \\
                                                       &= [4, 60, 59]%
                                                         \odot \left(%
                                                         \begin{bmatrix}
                                                           120 &  1\,390 &    658 \\
                                                           1\,360 & 13\,670 & 2\,874 \\
                                                           390 &  3\,930 &    846
                                                         \end{bmatrix} %
                                                         \times%
                                                         [0, 10, 22]%
                                                                           \right) \\
                                                       &= [4, 60, 59] \odot [28\,376, 199\,928,  57\,912] \\
                                                       &= [113\,504, 11\,995\,680,  3\,416\,808]
\end{align*}

Similarly, copy-subject diagonally places the subject dimension into a cube, producing a formula:
\begin{equation}
  \ov{\text{\it Obj}} \odot (\ov{\text{\it Verb}}^{\mathsf{T}} \times \ov{\text{\it Sbj}})
\end{equation}
%
Again, in the example:
\begin{align*}
  \overline{\text{\it John loves Mary}}_{\text{copy-subject}} &=\ov{\text{\it mary}} \odot (\overline{\text{\it loves}}^{\mathsf{T}} \times \ov{\text{\it john}}) \\
                                                        &= [0, 10, 22]%
                                                          \odot%
                                                          \left(%
                                                          \begin{bmatrix}
                                                               120 &  1\,360 &    390 \\
                                                            1\,390 & 13\,670 & 3\,930 \\
                                                               658 &  2\,874 &    846 \\
                                                          \end{bmatrix} %
                                                          \times%
                                                          [4, 60, 59]%
                                                          \right) \\
  \displaybreak[0]
                                                        &= [0, 10, 22] \odot [105\,090, 1\,057\,630,  224\,986] \\
                                                        &= [0, 10\,576\,300,  4\,949\,692]
\end{align*}
%
The series of contraction and multiplication simplifies to these closed forms, due to the special shape of the cube (that is created by copy-subject or copy-object). Note that the closed-form formulas do not include third order tensors.

\newcite{kartsadrqpl2014} observe that copy-object and copy-subject address a partial interaction of the verb with its arguments and propose a family of three \textit{Frobenius operators} that combine copy-object and copy-subject together.

Frobenius addition uses addition to combine the results:
\begin{equation}
  \big(\ov{\text{\it Sbj}} \odot (\overline{\text{\it Verb}} \times \ov{\text{\it Obj}})\big) + %
  \big(\ov{\text{\it Obj}} \odot (\overline{\text{\it Verb}}^{\mathsf{T}} \times \ov{\text{\it Sbj}})\big)
\end{equation}
%
So the computation of \textit{John loves Mary} is:
%
\begin{align*}
  \overline{\text{\it John loves Mary}}_{\text{Frobenius add}} &=  \overline{\text{\it John loves Mary}}_{\text{copy-object}} +  \overline{\text{\it John loves Mary}}_{\text{copy-subject}}\\
                                                         &= [113\,504, 11\,995\,680,  3\,416\,808] + [0, 10\,576\,300,  4\,949\,692] \\
                                                         &= [113\,504, 22\,571\,980,  8\,366\,500]
\end{align*}

Element-wise vector multiplication is used by Frobenius multiplication:% (Frob.~mult.~in Table~\ref{tbl:comp-methods})
\begin{equation}
  \big(\ov{\text{\it Sbj}} \odot (\overline{\text{\it Verb}} \times \ov{\text{\it Obj}})\big) \odot %
  \big(\ov{\text{\it Obj}} \odot (\overline{\text{\it Verb}}^{\mathsf{T}} \times \ov{\text{\it Sbj}})\big)
\end{equation}
%
Similarly, the computation of \textit{John loves Mary} becomes:
%
\begin{align*}
  \overline{\text{\it John loves Mary}}_{\text{Frobenius mult}} &=  \overline{\text{\it John loves Mary}}_{\text{copy-object}} \odot \overline{\text{\it John loves Mary}}_{\text{copy-subject}}\\
                                                         &= [113\,504, 11\,995\,680,  3\,416\,808] \odot [0, 10\,576\,300,  4\,949\,692] \\
                                                         &= [0, 126\,869\,910\,384\,000,  16\,912\,147\,223\,136]
\end{align*}

Finally, Kronecker product is used in Frobenius outer:% (Frob.~outer in Table~\ref{tbl:comp-methods}).
\begin{equation}
  \big(\ov{\text{\it Sbj}} \odot (\overline{\text{\it Verb}} \times \ov{\text{\it Obj}})\big) \otimes %
  \big(\ov{\text{\it Obj}} \odot (\overline{\text{\it Verb}}^{\mathsf{T}} \times \ov{\text{\it Sbj}})\big)
\end{equation}
%
Making the computation of \textit{John loves Mary} look like:
\begin{align*}
  \overline{\text{\it John loves Mary}}_{\text{Frobenius outer}} &=  \overline{\text{\it John loves Mary}}_{\text{copy-object}} \otimes \overline{\text{\it John loves Mary}}_{\text{copy-subject}}\\
                                                         &= [113\,504, 11\,995\,680,  3\,416\,808] \otimes [0, 10\,576\,300,  4\,949\,692] \\
                                                           &=%
                                                             \begin{bmatrix}
0 &   1\,200\,452\,355\,200 &      56\,1809\,840\,768 \\
0 & 126\,869\,910\,384\,000 &  59\,374\,921\,330\,560 \\
0 &  36\,137\,186\,450\,400 &  16\,912\,147\,223\,136 \\
                                                             \end{bmatrix}
\end{align*}


\subsection{Similarity of heads in context}
\label{sec:similarity-context}

A noun phrase can be similar to a noun, as in \textit{female lion} and \textit{lioness}, and to other noun phrases as in \textit{yellow car} and \textit{cheap taxi}. The same similarity principle can be applied to phrases as well as to words. In this case, similarity is measured in \emph{context},\footnotemark{} and most methods of calculating similarity of phrases still rely on comparisons of the phrases' head words, which meanings are modified by the arguments they appear with \cite{Kintsch2001173}.

\footnotetext{
  The similarity between the head words in the context of their phrases should not be confused with contextualised similarity. The similarity between heads specifies what is being compared. Contextualised similarity is a requirement on the similarity relation.
}

\newcite{mitchell-lapata:2008:ACLMain,mitchell2010composition} use element-wise addition and multiplication to model argument interaction. \newcite{Baroni2010nouns} represent adjectives as matrices that modify nouns (vectors) using matrix multiplication.

\newcite{Dinu:2010:MDS:1870658.1870771} model word meaning as a distribution over senses; a context feature (other word in the context)  directly modulates word’s sense distribution using conditional probability. \newcite{thater-furstenau-pinkal:2011:IJCNLP-2011} contextualise vectors by assigning higher weight to features that correspond or are distributionally similar to the context words.

% \cite{Seaghdha:2011:PMS:2145432.2145545}.

With verbs, similarity in context can be applied to compare a transitive verb with an intransitive verb. For example, \textit{cycle} is similar to \textit{ride a bicycle}. Here, we see that \textit{a bicycle} disambiguates the verb \textit{ride} making the phrase similar to the verb \textit{cycle}. The connection between measuring similarity of a phrase and disambiguation has been noted in \newcite{kartsaklis-sadrzadeh-pulman:2013:CoNLL-2013}.

% TODO: mention \cite{wieting2015paraphrase}

Sentential similarity might be treated as the similarity of the heads in their contexts. That is, the similarity between \textit{sees} and \textit{notices} in \textit{John \textbf{sees} Mary} and \textit{John \textbf{notices} a woman}. This approach abstracts away the grammatical difference between the sentences and concentrates on their semantics.


\section{Evaluation}
\label{sec:intrinsic-evaluation}

The difference in occurrence between two words quantifies the difference in their meaning \cite{harris1954distributional} and there is a procedure to measure the difference in the meanings of two sentences \cite{DBLP:journals/corr/abs-1003-4394}. The final necessary part is the evaluation methodology to test this approach.

Because it is difficult to perform extrinsic evaluation (also called evaluation in use) to measure the performance of a similarity component in a pipeline of a complete natural language processing system (for example, a dialog system), intrinsic datasets that focus on similarity are popular among computational linguists.

Apart from a pragmatic attempt to alleviate the problems of evaluating
similarity components, these datasets serve as an empirical test of the
hypotheses of Firth and Harris. They bring together our understanding of the human mind, language and technology. The following sections introduce the main datasets used.

\subsection{Word similarity}
\label{sec:lexical-similarity}

\newcite{Rubenstein:1965:CCS:365628.365657} performed an empirical study on the relationship between the similarity in word meaning and similarity of contexts they appear in. They build a list of 65 word pairs that range from highly synonymous to semantically unrelated.

To obtain the gold standard similarity judgements, the human subjects were given a shuffled deck of cards. Each card contained a word pair from the list. They were asked to sort the cards by similarity, so that the most similar items appeared in the top of the deck. In addition to ranking, human subjects were asked to give similarity scores ranging from 4.0 to 0.0, where the higher value indicates the higher similarity level.

To test the Distributional hypothesis, 100 sentences for each distinct word in the list were written by 50 participants who did not provide similarity judgements to be used as the contexts. The words from the list had to be used as nouns and the sentences had to be at least 10 words long.

To estimate similarity, the overlap was calculated over tokens and types. The token condition takes into account the appearance frequencies of context words, while the type condition only considers the word types. So if the word \textit{table} appeared 10 times as a context of a word of interest, in the case of the token condition all 10 occurrences are considered, in the case of the type condition only the fact that it appeared as a context is recorded.

Formally, the overlap $M_x$ over condition $x$ between the word context of words $A$ and $B$ was calculated as:
%
\begin{equation*}
  M_x = \frac{N(A_xB_x)}{\min(N(A_x), N(B_x))}
\end{equation*}
%
where $N(A_xB_x)$ is the number of context words shared between $A$ and $B$ under condition $x$. $N(A_x)$ and $N(B_x)$ are the number of context words under condition $x$ for the words $A$ and $B$ respectively.

They found that the more similar words in meaning are the more context they share for both token and type conditions. Moreover, the relationship is strongest for the highly synonymous pairs, namely the pairs with similarity greater than 3.0.

\newcite{1986-13502-00119860101} studied the similarity relation from the psychological perspective. They analysed the similarity judgements between the entries by their geometric structure. The geometric approach represents the entries as points in a multidimensional space so that the distance between them reflects similarity.

They examined 100 datasets to identify common geometric patterns in human similarity judgements. The datasets contained entries that belonged to a single category such as \textit{verbs of judging} \cite{FILLENBAUM197454} or \textit{animal terms} \cite{HENLEY1969176}. The reason for category oriented similarity studies is that ``stimuli can only be compared in so far as they have already been categorised as identical, alike, or equivalent at some higher level of abstraction'' \cite{turner1987rediscovering}.

They observed that if a category contains a superordinate, similarity judgements arrange category members around it. For example, similarity judgements given by humans arrange fruit names around the word \textit{fruit} in such a way that it is their nearest neighbour, making \textit{fruit} the \emph{focal point} of the category of \textit{fruits}.

An important consequence is that high centrality values cannot be achieved in a space with dimensionality of two or three,  because the dimensionality sets the upper bound on the number of points that can share the nearest neighbour.

\input{figures/lexical-dataset-comparison}

\newcite{2002:PSC:503104.503110} proposed a context oriented information retrieval framework. The idea that the meaning of the word \textit{jaguar} is dependent on the context the search is performed. It might be a car, if the query comes from an automotive website, or it might mean an animal if it comes from a website about nature. To evaluate their semantic component they developed a dataset WS353 that consists of 353 diverse noun pairs along with their relatedness scores on a scale from 0 (totally unrelated) to 10 (very much related or identical). The combination of a vector-based method and a WordNet-based method achieved correlation of 0.55. The dataset they proposed is widely used to evaluate algorithms that estimate semantic similarity.

\newcite{Bruni:2012:DST:2390524.2390544} introduced the MEN dataset to test a multimodal semantic space (the model used textual and visual features). The new dataset contains the words that appear in labels of the ESP-Game\footnote{\url{http://www.cs.cmu.edu/~biglou/resources/}} and MIRFLICKR-1M\footnote{\url{http://press.liacs.nl/mirflickr/}} image collections. Compared to WS353 it is sufficiently large to be split to the development part (2\,000 pairs) and to the test part (1\,000 pairs) for evaluation. The dataset contain highly similar items (\textit{cathedral}, \textit{church}, 0.94) and also terms that stand in a broader semantic relationship, such as whole-part (\textit{flower}, \textit{petal}, 0.92). The scores are relatedness scores on a scale from 0.0 to 1.0.

The Spearman correlation of the judgements given by two authors of the paper for all 3\,000 pairs is 0.68. The correlation with the average of their scores with the dataset scores is 0.84, which can be taken as the upper bound. The best presented results are 0.75 for WS353 and 0.78 for MEN, note that two different models produce them.

\newcite{hill2014simlex} presented SimLex-999, a gold standard resource for evaluating distributional semantic models. In contrast to WS353 and MEN, it focuses on similarity rather than relatedness. The words \textit{coffee} and \textit{cup} are related but not similar. The dataset consists of 666 noun-pairs, 222 verb-pairs and 111 adjective-pairs. 500 residents of the USA were recruited to collect human judgements. The average pairwise Spearman correlation between two participants is 0.67, however the average correlation of a human rater with the average of all other raters is 0.78.

They tested several models on WS353, MEN and WordSim-999. They showed that existing models achieved lower correlation on SimLex-999 than on WS353 and MEN suggesting that the new dataset required development of methods that are strictly focused on similarity. The best reported results in that study are 0.44 on WS353, 0.48 on MEN and 0.28 on SimLex-999, these are different models, but all of them are trained on the 150 million word RCV1 Corpus \cite{lewis2004rcv1}. Neural models trained on a larger corpus (Wikipedia) yield higher results as shown by \newcite{TACL570} and \newcite{baroni-dinu-kruszewski:2014:P14-1}.

Table~\ref{tab:lexical-dataset-comparison} presents the key models and the results they achieved on WS353, MEN and SimLex-999.

\subsection{Parameter selection for lexical models}
\label{sec:parameter-selection-intro}

The variance of results on word similarity tasks can be explained by the differences in algorithms or by difference in model parameters, corpus used and other factors. For example, \newcite{baroni-dinu-kruszewski:2014:P14-1} uses a corpus of 2.8 billion tokens and outperforms \newcite{hill2014simlex}, who uses a corpus of 0.15 billion tokens, on MEN by 0.28 points with a distributional model and by 0.37 with neural word embeddings.

\newcite{Bullinaria2007} presented a thorough study of parameters of distributional models. They tested various vector space parameters and similarity measures.

Instead of raw co-occurrence counts they used the conditional probability of a context word appearing close to a target word $P(c|t)$. In addition to that, they used \emph{point-wise mutual information} $\operatorname{PMI(c,t)} = \log\left(\frac{P(c,t)}{P(c)P(t)}\right)$ and positive PMI, which nullifies negative values, and probability ratio $\frac{P(c|t)}{P(c)}$. They varied the vector space dimensionality from 1 to 100\,000 dimensions.  Cosine, Euclidean and City Block geometric measures were used to estimate similarity. They used the 87.9 million word British National Corpus (BNC) and performed an exhaustive search across the parameter combinations.

A vector space with vector components computed with positive PMI and cosine similarity measure performed best in their experiments. They also showed that performance depends on the size of the corpus used: the larger the corpus, the better the results. The context window of size one produced the best results. The model performance peaked when 1\,000 dimensional vector space was used and dropped as dimensionality increased  afterwards.

\newcite{kiela-clark:2014:CVSC} performed a systematic study of distributional models on various datasets including WS353 and MEN. They confirmed findings of \newcite{Bullinaria2007} that larger corpora lead to better performance. They suggested using ukWaC (2 billion tokens) over the BNC with a small context window of size less than 5 from each side. Positive PMI was the best performed measure in combination with the \emph{correlation similarity} measure, which is the mean-adjusted cosine similarity. The dimensionality of 50\,000 appeared to be optimal.

\citet{kiela-clark:2014:CVSC} advocated incremental parameter tuning, reasoning that compositional tasks need a vector space model that is good for lexical tasks. Practically, this means that initially a good lexical model is identified and only then it is used to find a well performing compositional operator. \citet{BullinariaLevy2012} followed the same reasoning: first good sparse models are identified, and then dimensionality reduction methods are compared to find the best space with reduced dimensionality.

\citet{lapesa-evert:2013:CMCL} argue against incremental tuning because it does not capture parameter interaction. For example, a compositional operator might benefit from a specific weighting scheme, which is not necessarily the best in predicting word similarity. Their goal was to contrast rank-based prediction of semantic priming with distance-based prediction. Because they were introducing rank-based prediction, they could not reuse recommendations of parameters that are derived from distance-based similarity experiments. In their study, they covered a broad range of parameters of distributional models aiming at identification of parameter configurations that achieve good performance in predicting semantic priming.

They used a linear regression model to determine the importance of individual parameters and their combinations. The parameters of a distributional model---such as the weighting scheme, vector space dimensionality and similarity metric---were considered predictors of its performance: the parameters of a distributional model were independent parameters of a linear model and the performance of the distributional model was a dependent variable. Analysis of variance was used to determine the most important parameters and their interactions.

They showed that a statistical association measures---such as t-score or z-score \cite{Evert05}---with the combination of ranked-based prediction yielded better results over cosine similarity. Would they have performed only iterative tuning, they would test ranked-based prediction only with the PMI weighting, which underperformed in their experiments.

In a successive study that included word similarity, \citet{lapesa2014large} showed that the combination of the similarity score together with the weighting scheme was the most important parameter combination, supporting the findings of \newcite{Bullinaria2007} and \newcite{kiela-clark:2014:CVSC}, who suggested to use PMI together with cosine similarity.

%With their analysis they were also to perform model selection that is robust to overfitting. In most cases, the result of their robust parameter optimisation was close to the highest score. They considered the highest score they achieved to be overfitted because 537600 models were tested and there was a high chance that the best performance happened by chance.

\newcite{baroni-dinu-kruszewski:2014:P14-1} and \newcite{TACL570} performed systematic parameter studies on distributional and neural models. \newcite{TACL570} evaluated models on all three datasets (WS353, MEN and SimLex-999, see Table~\ref{tab:lexical-dataset-comparison}). Notably, their best distributional model outperformed the distributional models presented in \citet{hill2014simlex}, \citet{kiela-clark:2014:CVSC} and \citet{baroni-dinu-kruszewski:2014:P14-1} and to the best of our knowledge is the state of the art of distributional models.

\subsection{Statistical significance testing}
\label{sec:stat-sign}

\citet{rastogi-vandurme-arora:2015:NAACL-HLT} proposed and \citet{W16-2506} expressed the importance of a method for significance testing motivated by the fact that the researchers do not report measures of significance of the difference between the Spearman correlations. Their method is based on finding a minimal required difference for significance (MRDS).

Consider two lists of ratings over the same dataset: $A$ and $B$ produced by two competing models together with a list of gold ratings $T$. Let $r_{AT}$, $r_{BT}$, and $r_{AB}$ be the Spearman correlations between $A{:}T$, $B{:}T$ and $A{:}B$ respectively. Let $\hat{r}_{AT}$, $\hat{r}_{BT}$ and $\hat{r}_{AB}$ be their empirical estimates and assume without loss of generality that $\hat{r}_{BT}>\hat{r}_{AT}$.

The MDRS of a dataset $\sigma^{r}_{p_0}$ is defined such that it satisfies the following:
\begin{equation}
(r_{AB} < r) \wedge (|\hat{r}_{BT} - \hat{r}_{AT}| < \sigma^r_{p_0} \Rightarrow \mathit{pval} > p_0
\end{equation}

In the constraint above, $\mathit{pval}$ is the probability of the test statistic under the null hypothesis that $r_{AT} = r_{BT}$ found using Staiger's test \cite{steiger1980tests}.

\begin{table}
  \centering
  \begin{tabular}[th]{rl}
    \toprule
    Dataset & $\sigma^{0.9}_{0.05}$ \\
    \midrule
    SimLex-999 & 0.023\\
    MEN  & 0.013 \\
    KS14 &  0.07 \\
    GS11 &  0.05 \\
    PhraseRel & 0.43 \\
    \bottomrule
  \end{tabular}
  \caption[MRDS numbers]{Minimal required difference for significance (MRDS) numbers for the datasets used in this work. Values for SimLex-999 and MEN are taken from \citet{rastogi-vandurme-arora:2015:NAACL-HLT}. KS14, GS11 and PhraseRel are described later in this work.}
  \label{tab:mrds}
\end{table}

The constraint ensures that the $p$value of the null hypothesis will be greater than $p_0$ given that the correlation between the methods is less than $r$ and the difference between the correlations of the competing models to the gold standard is less then $\sigma^r_{p_0}$.

The value of $r$ specifies the upper bound on the agreement of the ratings produced by the competing models, for example $r = 0.9$.

The second part of the predicate ensures that the null hypothesis is more likely than $p_0$ given that the difference between the correlations of the models to the gold standard is less than $\sigma^r_{p_0}$.

Once a reasonable value of $r$ is chosen (for example 0.9), $\sigma^r_{p_0}$ can be found following this procedure. Let $\operatorname{stest}$ be Staiger's test predicate which satisfies the following ($n$ being the size of the dataset):
\begin{equation}
  \label{eq:stest}
  \operatorname{stest-p}(\hat{r}_{AT}, \hat{r}_{BT}, r_{AB}, p_0, n) \Rightarrow \mathit{pval} < p_0
\end{equation}

Now it is possible to search for $\sigma^r_{p_0}$ by solving:
\begin{equation}
  \label{eq:mrsd-min}
  \sigma^r_{p_0} = \min\{\sigma | \forall_{0 < r' < 1} \operatorname{stest-p}(r', \min(r'+\sigma, 1), r, p_0, n) \}
\end{equation}

The $\sigma^{0.9}_{0.05}$ values for the lexical datasets and phrasal datasets (see next section and Chapter~\ref{sec:phraserel}) are listed in Table~\ref{tab:mrds}.

\subsection{Disambiguation of verbs in context}
\label{sec:disamb}

The transitive verb disambiguation dataset (GS11)
% \footnote{This and the sentence  similarity datasets are available at \url{http://www.cs.ox.ac.uk/activities/compdistmeaning/}}
described in \citet{Grefenstette:2011:ETV:2140490.2140497,Grefenstette:2011:ESC:2145432.2145580} consists of ambiguous transitive verbs together with their arguments, landmark verbs (which identify one of the verb senses) and human judgements (which specify the similarity to the landmarks of the disambiguated sense of the verb in the given context). This is similar to the intransitive dataset described in \newcite{mitchell-lapata:2008:ACLMain}.

Consider the sentence \textit{system meets specification}. \textit{Meets} is an ambiguous transitive verb, and \textit{system}
and \textit{specification} are its arguments. Possible landmarks for \emph{meet} are \textit{satisfy} and \textit{visit}. For this sentence, the human judgements show that the disambiguated verb meaning is similar to the landmark \textit{satisfy}, and less similar to \textit{visit}.

The task is to estimate the similarity of the sense of a verb in a context with a given landmark. To estimate similarity, the verb is composed with its arguments, it is done the same for the landmark and the arguments, and the similarity of the two phrase vectors is computed. To evaluate performance, the human judgements are averaged for the same verb, argument and landmark entries, and these average values are used to calculate the correlation. \citet{Grefenstette:2011:ETV:2140490.2140497} achieve the correlation of 0.28 with the composition based on Kronecker.

\subsection{Sentence similarity}
\label{sec:sentence-similarity}

The transitive sentence similarity dataset (KS14) described in \newcite{kartsaklis-sadrzadeh-pulman:2013:CoNLL-2013,kartsadrqpl2014} consists of transitive sentence pairs and human similarity judgements. The task is to estimate similarity between two sentences. The evaluation is the same as in the disambiguation task (Section~\ref{sec:disamb}). They achieve the correlation of 0.41 with additive composition.

\subsection{Parameter selection for compositional models}
\label{sec:comp-parameter-selection}

In general, there is no systematic study of parameters of compositional models similar to lexical studies discussed in Section~\ref{sec:parameter-selection-intro}. First of all, composition brings another type of parameters, so not only lexical representations have to be optimised, but also an optimal way of composition has to be found.

Early compositional studies focused on the compositional operators, using lexical representations that are good for lexical tasks. Because parameters were selected iteratively, the fact that one operator outperformed other could be due to the specificity of lexical representations. Nevertheless, experiments based on iterative parameter selection gave positive results, see, for example, the papers that introduced the compositional datasets described above \citet{Grefenstette:2011:ETV:2140490.2140497,kartsaklis-sadrzadeh-pulman:2013:CoNLL-2013}.

\citet{blacoe2012comparison} is---to the best of our knowledge---the first paper that, apart from identifying the best compositional operator, also  explicitly contrasted two kinds of lexical representations: shallow that are based on the co-occurrence of words and embeddings that are learned by a neural language model \cite{bengio2006,Collobert:2008:UAN:1390156.1390177}. They concluded that shallow approaches are as good as their computationally more intensive counterparts based on language models in phrase similarity and paraphrase detection tasks. They also identified the importance of the combination of lexical representation and the compositional method.

\citet{milajevs-EtAl:2014:EMNLP2014}, the work that set the ground for this thesis, %also contrasts global and local representations (Section~\ref{sec:distributional-representations})
also contrasts count and predict \cite{baroni-dinu-kruszewski:2014:P14-1} models
in compositional setting. However, in contrast to \citet{blacoe2012comparison} experiments, prediction-based models showed robust performance yielding better results on a number of tasks and were recommended for usage in compositional experiments.
%
%In the literature, local and global models are also referred as count-based models and prediction-based models respectively, after a paper with a flashy title ``Don’t count, predict!~A systematic comparison of context-counting vs.~context-predicting semantic vectors'' by \citet{baroni-dinu-kruszewski:2014:P14-1}.
%
The work consecutive to \citet{milajevs-EtAl:2014:EMNLP2014} (consequently or coincidentally) focuses on predictive lexical representations.

The CBOW model of local lexical representations \cite{mikolov2013linguistic,mikolov2013distributed,mikolov2013efficient} 
% (Section~\ref{sec:distributional-representations})
implicitly assumes additive interaction between context words to predict the target word. However, categorical compositional operators (Section~\ref{sec:composition}) make use of multiplication as a part of composition. This inconsistency was noticed in \citet{kim2015neural}. They changed the objective function
% \eqref{eq:objective-func-cbow}
to include multiplication:
%
\begin{equation}
 \frac{1}{T}\sum^{T}_{t=1}\prod_{-c \leq j \leq c, j\neq0} \log P(w_{t}|w_{t+j})
  \label{eq:objective-func-cbow-mult}
\end{equation}
%
This change improved results of categorical compositional methods on the sentence similarity dataset (Section~\ref{sec:sentence-similarity}), but not on the verb disambiguation task (Section~\ref{sec:disamb}).

In the categorical framework, the result of composition depends on how the tensors for transitive verbs are constructed. The most straightforward approach is to represent transitive verbs as matrices (saving a dimension) as it is shown in (\ref{eq:verb-matrix}) on page~\pageref{eq:verb-matrix}. However, the number of elements in these matrices is still high and equal to the square of the number of elements in vectors for nouns. To cope with this problem various solutions were tried: linear-regression of the full tensors \cite{grefenstette-et-al:2013:IWCS2013}, the combination of regression with a plausibility space \cite{polajnar-clark:2014:EACL} and low-rank tensor approximation \cite{fried-polajnar-clark:2015:ACL-IJCNLP}. While these methods not necessarily lead to the highest results, they considerably reduce the amount of data associated with lexical entries.

Finally, \citet{hashimoto-tsuruoka:2015:CVSC,hashimoto-EtAl:2014:EMNLP2014} proposed a tensor factorisation method that directly models the interaction between predicates and their arguments. The intuition is that the arguments should be able to disambiguate the meaning of their predicate.

\subsection{Extrinsic evaluation}

It is common to evaluate word meaning representations and compositional models in a pipeline, for example dialogue act tagging \cite{Stolcke:2000:DAM:971869.971872} or paraphrase detection \cite{dolan2005par}. However, these datasets contain grammar which is currently in practice not covered by categorical compositional method of \citet{DBLP:journals/corr/abs-1003-4394}.

Apart from handling a richer set of grammatical structures, the datasets contain a larger vocabulary, meaning that there are, for instance, more verbs for which matrices are needed to be build. Given an exhaustive experiments over a large number of parameter combinations it was not feasible to conclude such a study.

\section{Conclusion}
\label{sec:conclusion-background}

Similarity is an important notion in psychological theories of knowledge and behaviour. It is also useful in explaining language. Many NLP systems benefit from internal similarity components. However the exact definition of similarity is task-dependent: an IR system needs to know whether the words are related (for example, the verb \textit{swim} is related to the noun \textit{lake}), language models benefit from semantic similarity (the noun \textit{lake} is similar to the noun \textit{reservoir}, but not to the verb \textit{swim}), dialog systems need to know the similarity of utterances by the role they play in  discourse: \textit{Hi!} and \textit{Good morning.} are both greetings, for example.

Similarity measurement of words and multi-word units is theoretically challenging. \newcite{goodman1972problems} argues that the similarity relation does not exist, because everything can be shown to be similar to everything else. \newcite{medin1993respects} and \newcite{Markman1996} response that similarity has to be contextualised to be measured, that is, the features of interest need to be defined before the measurement.

According to \newcite{harris1954distributional}, the word meaning does not need to be obtained to measure similarity, instead, the differences of occurrences of two words quantify the difference in their meaning. In this way, the problems with representing meaning in isolation raised by Frege are avoided, because word meaning is not constructed.

The principle of compositionality, that the representations of compounds are built from their parts, is the hallmark of categorical compositional semantics \cite{DBLP:journals/corr/abs-1003-4394}. It extends the composition mechanism from formal semantics by replacing the symbolic representation of atoms and relations with tensors of various orders.

In categorical grammars, the meaning of a phrase is obtained by applying backward and forward application rules. Consider a phrase \textit{John walks}. In this example, the string \textit{John} is the atom $\mathit{John'}$ of category $\mathtt{np}$ and type $e$, \textit{walks} is the relation $\lambda x.\mathit{walks'}(x)$ of category $\mathtt{np\backslash{}s}$ and type $(e \to t)$. During the phrase composition, the backward application rule is applied, so the category of the whole string becomes $\mathtt{s}$, its type is $t$ which is either \textit{true} or \textit{false} and the meaning is computed by the evaluation of the formula $\mathit{walks'}(\mathit{John'})$, basically the atom $\mathit{John'}$ is applied to the formula $\lambda x.\mathit{walks'(x)}$.

Categorical compositional semantics replaces atomic symbols in formal semantics with vectors (first-order tensors), and relations with higher-order tensors (so \textit{walks} is represented by a second-order tensor, which is a matrix). To obtain the representation of a compound, tensor contraction is used instead of function application.

Word similarity (in the broad psychological sense) has been studied extensively: many datasets were proposed that focus on relatedness and similarity (in the specific sense that \textit{lake} is similar to \textit{reservoir}, but is not similar to \textit{swim}). Several studies were carried out to identify the best parameter choice for the models of similarity.

Iterative parameter tuning is widely used to find the best model parameters. However, it does not take into account the interaction between the parameters of a similarity model. While some influential interactions are well known (for example, the weighting scheme interacts with the similarity score) and specific parameter choice is recommended (Positive PMI with cosine similarity), studies that introduce new parameters should not rely on existing recommendations, because an effective parameter combination might not be tested.

Phrase similarity has been studied much less than word similarity. Several datasets are proposed that focus on different aspects of similarity (for example, its application in word sense disambiguation within a context). Several compositional methods are developed. Other studied directions are the interaction of the lexical representations and compositional method and approximation of predicates to reduce the size of their representations.

However, the compositional methods were not evaluated in a systematic way to identify the best parameters (for example, the co-occurrence weighting function among the others) and rely on iterative tuning by taking as a starting point parameters that work best with words. This might lead to biased evaluation.

This thesis addresses the gap between the evaluation methodology of lexical and compositional similarity models. It is a systematic study of both words and phrasal similarity models. It covers a broad selection of parameter combinations that have not been tested before, especially on the phrase similarity tasks. It uses several datasets and employs a model selection methodology that is robust to overfitting.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis.tex"
%%% TeX-engine: xetex
%%% End:
