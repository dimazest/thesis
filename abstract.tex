{\Large \headingfont \thetitle}

\vspace{1em}

{\large \headingfont \theauthor}

\vspace{1em}

{\headingfont Abstract}

Representation of sentences that captures semantics is an essential part of natural language processing systems, such as information retrieval or machine translation. The representation of a sentence is commonly built by combining together the representations of words that the sentence consists of. Similarity between words is used as a proxy to evaluate semantic representations. The word similarity models are well studied and correlate with human judgements.

Current evaluation of models of sentential similarity builds on the results obtained in lexical experiments. The main focus is how the lexical representations are used, rather than what they should be. It is often assumed that the optimal representations for word similarity are also optimal for sentence similarity.

This work drops this assumption and systematically looks for lexical representations that are optimal for similarity measurement. We find that the best representations for words similarity are not always the best for sentence similarity and vice versa. However, there are representations that are good in both tasks. The systematic study of the parameters of similarity models reveals that the more information lexical representations contain, the more attention should be paid to noise.

\vfill

%Submitted in partial fulfillment of the requirements of the Degree of Doctor of Philosophy

Submitted for the degree of Doctor of Philosophy

Queen Mary University of London

\today


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
